{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944df786",
   "metadata": {},
   "source": [
    "### Topic Modelling \n",
    "\n",
    "Overall Research Questions:\n",
    "1) How do customers define 'good' service, and how does the new script shift those definitions?\n",
    "2) What aspects of service (clarity, empathy, agent personality) drive variance in sentiment?\n",
    "3) Does the new script systematically change perceptions or emotional tone, particularly for high-value segments like VOLT?\n",
    "\n",
    "What are the best topic modelling approaches given the above?\n",
    "\n",
    "In addition:\n",
    "- What are the top latent topics mentioned?\n",
    "- How do topic distributions differ by treatment?\n",
    "- What percentage of comments mention agent personality, clarity, or reassurance?\n",
    "\n",
    "Topic modelling is most relevant to RQ1, but relates to RQ3 as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251166c7",
   "metadata": {},
   "source": [
    "#### Load the data/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7d138d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main dataset loaded: (582, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exec(open('../scripts/setup.py').read()) # load our data/packages\n",
    "# Text preprocessing functions are already defined in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca905a63",
   "metadata": {},
   "source": [
    "### Approach 1: Preprocessing + Topic Extraction (via BERTopic) - EXPERIMENTAL SECTION\n",
    "\n",
    "**NOTE: This section contains experimental code that was not used in the final pipeline. The analysis showed that rule-based topic classification (Approach 2) was more suitable for the business questions. This code is preserved for reference and potential future development.**\n",
    "\n",
    "Here we'll try a combination of pre processing steps to try to prepare the feedback text with the goal of extracting substantively meaningful topics with a BERTopic Model that would help answer our research questions.\n",
    "\n",
    "**Why BERTopic?**\n",
    "\n",
    "Through the use of transformers, it is robust to typos (a common trait we saw previously) and it provides a adaptive algorithm for finding semantic/definitional similarity between words. This is better than other topic modelling architectures (like LSA) that perform poorly on short documents and relies on term-document matrices that may become very sparse. LDA relies on word co-occurence patterns within documents so might also struggle with short texts. Since we ultimately want to predict sentiment with topics, we need the ability to extract on the short-texts.  \n",
    "\n",
    "**After Experimentation: (added after doing the below)**\n",
    "\n",
    "After the below steps, I've now seen that the preprocessing involved for the BERTopic model is quite involved, as the input drastically changes the topics extracted. Since the model is applying unsupervised measurement, it is quite sensitive to adjectives and certain stop-words that are often difficult to robustly filter in preprocessing. The topics extracted weren't very useful from a substantive perspective and would be defined as 'agent_everything_package_service' or 'explained_everything_well_fully'. While these are initially useful, I'm adding Approach 2 alongside Approach 1 to get more concrete topic extraction based on different categories, with methodology detailed in that section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "587549c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GROUP</th>\n",
       "      <th>VOLT_FLAG</th>\n",
       "      <th>SURVEY_ID</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>LTR_COMMENT</th>\n",
       "      <th>PRIMARY_REASON</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>CONNECTION_TIME</th>\n",
       "      <th>SALES_PERSON_SAT</th>\n",
       "      <th>SALES_FRIENDLY_SAT</th>\n",
       "      <th>COMMINICATION_SAT</th>\n",
       "      <th>FIRST_BILL_SAT</th>\n",
       "      <th>AGENT_KNOWLEDGE</th>\n",
       "      <th>VOLT_FLAG_BINARY</th>\n",
       "      <th>TREATMENT_BINARY</th>\n",
       "      <th>LTR_COMMENT_CLEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>control</td>\n",
       "      <td>NaN</td>\n",
       "      <td>352240580</td>\n",
       "      <td>10</td>\n",
       "      <td>Good package</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good package</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>control</td>\n",
       "      <td>yes</td>\n",
       "      <td>351664275</td>\n",
       "      <td>10</td>\n",
       "      <td>Very good customer service</td>\n",
       "      <td>Customer Service,General,UK Legacy</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Very good customer service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>control</td>\n",
       "      <td>yes</td>\n",
       "      <td>351723391</td>\n",
       "      <td>10</td>\n",
       "      <td>So far so good. Charlie was very efficient and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>10</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>So far so good. Charlie was very efficient and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>control</td>\n",
       "      <td>NaN</td>\n",
       "      <td>351702901</td>\n",
       "      <td>10</td>\n",
       "      <td>Great communication</td>\n",
       "      <td>Customer Service,General,UK Legacy</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Great communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>control</td>\n",
       "      <td>yes</td>\n",
       "      <td>352243612</td>\n",
       "      <td>10</td>\n",
       "      <td>Because Chris was amazing when she contacted m...</td>\n",
       "      <td>Customer Service,UK Legacy</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>10</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Because Chris was amazing when she contacted m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      GROUP VOLT_FLAG  SURVEY_ID  SCORE                                        LTR_COMMENT                      PRIMARY_REASON      MONTH  CONNECTION_TIME  SALES_PERSON_SAT  SALES_FRIENDLY_SAT  COMMINICATION_SAT  FIRST_BILL_SAT  AGENT_KNOWLEDGE  VOLT_FLAG_BINARY  TREATMENT_BINARY                                  LTR_COMMENT_CLEAN\n",
       "45  control       NaN  352240580     10                                       Good package                                 NaN 2023-03-01               10                10                   8                 10              10               10                 0                 0                                       Good package\n",
       "46  control       yes  351664275     10                         Very good customer service  Customer Service,General,UK Legacy 2023-03-01               10                10                  10                 10              10               10                 1                 0                         Very good customer service\n",
       "47  control       yes  351723391     10  So far so good. Charlie was very efficient and...                                 NaN 2023-03-01               10              <NA>                  10                 10              10               10                 1                 0  So far so good. Charlie was very efficient and...\n",
       "48  control       NaN  351702901     10                                Great communication  Customer Service,General,UK Legacy 2023-03-01                9                10                  10                 10              10               10                 0                 0                                Great communication\n",
       "49  control       yes  352243612     10  Because Chris was amazing when she contacted m...          Customer Service,UK Legacy 2023-03-01               10              <NA>                  10                 10              10               10                 1                 0  Because Chris was amazing when she contacted m..."
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f090f9d",
   "metadata": {},
   "source": [
    "#### Let's randomly sample some text responses to build a suitable approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "689d6891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: Very helpful staff\n",
      "\n",
      "Sample 2: Very polite engineer who consulted every step and very patient\n",
      "\n",
      "Sample 3: The experience is really good. The price can be a bit expensive. But I have had good experience with company media so far. The engineer, who came to my address was very professional and friendly. And did a really good job running the wires nice and tidy. Exactly how I wanted.\n",
      "\n",
      "Sample 4: Good value reasonable prices\n",
      "\n",
      "Sample 5: Very efficient and helpful\n",
      "\n",
      "Sample 6: The lady I spoke to was very helpful.\n",
      "\n",
      "Sample 7: Because it's too fast\n",
      "\n",
      "Sample 8: Just felt treated better than with sky\n",
      "\n",
      "Sample 9: Your customer service is awful and you really need to ditch the overseas call centers. Kindness and understanding in the voice go a long way. You can feel the contempt in operator voices (sniggering and laughing when you say you cannot understnd what they are saying - feels intentional) and cannot quite get all of the words due to very strong accents. Your only saving grace is the physical connection and stability of that. Thankfully, because connection quality is good I rarely (if ever - Thank Goodness) have to contact your call centers. Would not touch with a barge pole otherwise. Take a look at Now Broadband whos operators are absolutely marvellous. Lessons learned or brushed under the carpet? Haha\n",
      "\n",
      "Sample 10: Very friendly staff and accommodating\n",
      "\n",
      "Sample 11: The person who talked me through what was going to happen was very patient and talked me through all that wÃ¡s happening.\n",
      "\n",
      "Sample 12: Straight forward\n",
      "\n",
      "Sample 13: Friendly helpful\n",
      "\n",
      "Sample 14: Guys who rang me was very pleasant and he explained exactly what he was going to be doing as I wasn't at home at the time.\n",
      "\n",
      "Sample 15: The team who came out were friendly and explained all they needed to do. Also clear about the fact one of them was a trainee but still provided an excellent customer service\n",
      "\n",
      "Sample 16: Customer service excellent,polite and extremely helpful .\n",
      "\n",
      "Sample 17: I am new to company media so I can't say yet but I liked the service when setting up my account.\n",
      "\n",
      "Sample 18: The service was fabulous,\n",
      "\n",
      "Sample 19: Prompt response Swift delivery Good network Affordability\n",
      "\n",
      "Sample 20: company media is the worst company I have ever used you cut the phone off with out any notice so we only went bk with your terrible company to keep our phone number witch you promised we would and you even made a mess av that so now you've given us a new number absolutely disgusting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# randomly print a sample from non-missing LTR_COMMENT in df\n",
    "sample_size = 20\n",
    "sample = df[df['LTR_COMMENT'].notna()]['LTR_COMMENT'].sample(n=sample_size, random_state=46).tolist()\n",
    "for i, comment in enumerate(sample, 1):\n",
    "    print(f\"Sample {i}: {comment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8fdee",
   "metadata": {},
   "source": [
    "Comments comprise long and short responses. Topic modelling is more nuanced for longer responses, even though short responses can reflect similar (but more simple) themes. THe model is highly sensitive to its inputs.\n",
    "\n",
    "A suitable first step for preprocessing is to clean the responses to remove punctuation and unusual characters. We just want the words that convey meaningful information about the **topic** of the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "8ba3f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the following as a module later on\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)  \n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "def clean_text(text, return_tokens=False, min_word_length=2, language='english'):\n",
    "    \"\"\"\n",
    "    Clean a single text string for NLP analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text string to clean\n",
    "    return_tokens : bool, default False\n",
    "        If True, returns list of tokens; if False, returns cleaned text string\n",
    "    min_word_length : int, default 2\n",
    "        Minimum word length to keep\n",
    "    language : str, default 'english'\n",
    "        Language for stopwords\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str or list\n",
    "        Cleaned text string or list of tokens\n",
    "    \"\"\"\n",
    "    # Initialize components\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Handle missing/empty text\n",
    "    if pd.isna(text) or not str(text).strip():\n",
    "        return [] if return_tokens else ''\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove punctuation and normalize whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize, filter stopwords and short words\n",
    "    tokens = [word for word in text.split() \n",
    "              if word not in stop_words and len(word) >= min_word_length]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens if return_tokens else ' '.join(tokens)\n",
    "\n",
    "def clean_text_series(series, return_tokens=False, min_word_length=2, language='english'):\n",
    "    \"\"\"\n",
    "    Apply text cleaning to a pandas Series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pandas.Series\n",
    "        Series containing text data\n",
    "    return_tokens : bool, default False\n",
    "        If True, returns list of tokens; if False, returns cleaned text string\n",
    "    min_word_length : int, default 2\n",
    "        Minimum word length to keep\n",
    "    language : str, default 'english'\n",
    "        Language for stopwords\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.Series\n",
    "        Series with cleaned text\n",
    "    \"\"\"\n",
    "    return series.apply(lambda x: clean_text(x, return_tokens, min_word_length, language))\n",
    "\n",
    "def add_cleaned_text_columns(df, text_columns, suffix='_clean', \n",
    "                           min_word_length=2, language='english', \n",
    "                           add_tokens=True, add_word_count=True):\n",
    "    \"\"\"\n",
    "    Add cleaned text columns to dataframe for multiple text columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe\n",
    "    text_columns : str or list\n",
    "        Column name(s) containing text to clean\n",
    "    suffix : str, default '_clean'\n",
    "        Suffix for cleaned text columns\n",
    "    min_word_length : int, default 2\n",
    "        Minimum word length to keep\n",
    "    language : str, default 'english'\n",
    "        Language for stopwords\n",
    "    add_tokens : bool, default True\n",
    "        Whether to add tokenized version\n",
    "    add_word_count : bool, default True\n",
    "        Whether to add word count column\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Dataframe with additional cleaned text columns\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Ensure text_columns is a list\n",
    "    if isinstance(text_columns, str):\n",
    "        text_columns = [text_columns]\n",
    "    \n",
    "    for col in text_columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Add cleaned text column\n",
    "        clean_col = f\"{col}{suffix}\"\n",
    "        df_result[clean_col] = clean_text_series(\n",
    "            df[col], return_tokens=False, \n",
    "            min_word_length=min_word_length, language=language\n",
    "        )\n",
    "        \n",
    "        # Add tokenized version\n",
    "        if add_tokens:\n",
    "            tokens_col = f\"{col}_tokens\"\n",
    "            df_result[tokens_col] = clean_text_series(\n",
    "                df[col], return_tokens=True,\n",
    "                min_word_length=min_word_length, language=language\n",
    "            )\n",
    "        \n",
    "        # Add word count\n",
    "        if add_word_count:\n",
    "            count_col = f\"{col}_word_count\"\n",
    "            df_result[count_col] = df_result[clean_col].str.split().str.len().fillna(0)\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "# Convenience function for common preprocessing pipeline\n",
    "def preprocess_text_for_modeling(df, text_columns, min_word_length=2, \n",
    "                                language='english', filter_empty=True):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for text modeling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe\n",
    "    text_columns : str or list\n",
    "        Column name(s) containing text to clean\n",
    "    min_word_length : int, default 2\n",
    "        Minimum word length to keep\n",
    "    language : str, default 'english'\n",
    "        Language for stopwords\n",
    "    filter_empty : bool, default True\n",
    "        Whether to add flag for non-empty text\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Preprocessed dataframe ready for modeling\n",
    "    \"\"\"\n",
    "    # Clean text\n",
    "    df_clean = add_cleaned_text_columns(\n",
    "        df, text_columns, min_word_length=min_word_length, \n",
    "        language=language, add_tokens=True, add_word_count=True\n",
    "    )\n",
    "    \n",
    "    # Add flags for non-empty text\n",
    "    if filter_empty:\n",
    "        if isinstance(text_columns, str):\n",
    "            text_columns = [text_columns]\n",
    "        \n",
    "        for col in text_columns:\n",
    "            clean_col = f\"{col}_clean\"\n",
    "            flag_col = f\"has_{col}\"\n",
    "            df_clean[flag_col] = (df_clean[clean_col].str.len() > 0)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "343a07ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LTR_COMMENT_CLEAN'] = clean_text_series(df['LTR_COMMENT'], return_tokens=False, min_word_length=2, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e077832f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ORIGINAL vs CLEANED COMMENTS ===\n",
      "\n",
      "--- Sample 1 ---\n",
      "ORIGINAL: Percy explained things clearly and understood from the start what I wanted. there was no hard sell. there was listening and understanding. top marks Percy and thank you\n",
      "CLEANED:  percy explained thing clearly understood start wanted hard sell listening understanding top mark percy thank\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 2 ---\n",
      "ORIGINAL: Excellent service lovely representative Mathius\n",
      "CLEANED:  excellent service lovely representative mathius\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 3 ---\n",
      "ORIGINAL: Very friendly staff and accommodating\n",
      "CLEANED:  friendly staff accommodating\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 4 ---\n",
      "ORIGINAL: Very difficult to talk to anyone, very expensive, always an IT problem, too many calls, lots of mistakes took place with my name, dates leaving or moving in, misleading advertising in such that it appears cheaper than what it actually costs. Sorry if that is too negative, however this is an accurate reflection of company. You do however have fast and mostly reliable internet\n",
      "CLEANED:  difficult talk anyone expensive always problem many call lot mistake took place name date leaving moving misleading advertising appears cheaper actually cost sorry negative however accurate reflection company however fast mostly reliable internet\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 5 ---\n",
      "ORIGINAL: Your customer service was so good to me\n",
      "CLEANED:  customer service good\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 6 ---\n",
      "ORIGINAL: Perfect\n",
      "CLEANED:  perfect\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 7 ---\n",
      "ORIGINAL: Very efficient and Friendly Service\n",
      "CLEANED:  efficient friendly service\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 8 ---\n",
      "ORIGINAL: Jake the representative for company was simply above and beyond helpful and done everything for me with ease\n",
      "CLEANED:  jake representative company simply beyond helpful done everything ease\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 9 ---\n",
      "ORIGINAL: Excellent service from the get go, the gent I spoke to on the phone, and the guy who did the installation were superb. I can¬\"t remember their name though.\n",
      "CLEANED:  excellent service get go gent spoke phone guy installation superb remember name though\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 10 ---\n",
      "ORIGINAL: All requirements were met with courtesy and helpfulness and promptness. I was informed of all I needed to know.\n",
      "CLEANED:  requirement met courtesy helpfulness promptness informed needed know\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Simple side-by-side comparison of original vs cleaned comments\n",
    "sample_size = 10\n",
    "\n",
    "# Get sample of non-missing comments\n",
    "sample_df = df[df['LTR_COMMENT'].notna()].sample(n=sample_size, random_state=42)\n",
    "\n",
    "print(\"=== ORIGINAL vs CLEANED COMMENTS ===\\n\")\n",
    "\n",
    "for i, (idx, row) in enumerate(sample_df.iterrows(), 1):\n",
    "    print(f\"--- Sample {i} ---\")\n",
    "    print(f\"ORIGINAL: {row['LTR_COMMENT']}\")\n",
    "    print(f\"CLEANED:  {row['LTR_COMMENT_CLEAN']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed912a06",
   "metadata": {},
   "source": [
    "The above applies lammatization, removes standard stopwords and short words from a the natural language toolkit approach. \n",
    "\n",
    "We can see in the above that the topic is more distilled down in these cases. However, it is still not very clear. There are a number of remaining questions. What do we do with verbs? Do we just extract nouns? What about the adjectives related to the customer service provider?\n",
    "\n",
    "All of the above examples are informed, but they aren't necessarily filtered enough to be very useful. We'll come back to this in approach 2.\n",
    "\n",
    "Later on, we could also explore lemmatization and stemming, which would further enable generalizability between comments. However, BERTopic is generally pretty robust even without these steps, through its use of transformers that do pretty well at finding similar word meanings/connotations. \n",
    "\n",
    "For now, we'll proceed with BERTopic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118f0d6",
   "metadata": {},
   "source": [
    "#### BERTopic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "68894326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (582, 16)\n",
      "Clean data shape: (349, 16)\n",
      "Number of comments for topic modeling: 349\n"
     ]
    }
   ],
   "source": [
    "# BERTopic Implementation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Load your data (assuming df is already loaded with LTR_COMMENT_CLEAN column)\n",
    "# df = pd.read_pickle('../data/processed/cleaned_call_script_data.pkl')\n",
    "\n",
    "# Step 1: Data preparation - filter out null/missing comments\n",
    "df_clean = (df\n",
    "    .loc[df['LTR_COMMENT_CLEAN'].notna()]  # Remove missing comments\n",
    "    .loc[df['LTR_COMMENT_CLEAN'].str.len() >= 20]  # Remove very short comments\n",
    "    #.reset_index(drop=True)  # Reset index after filtering\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "print(f\"Original data shape: {df.shape}\")\n",
    "print(f\"Clean data shape: {df_clean.shape}\")\n",
    "print(f\"Number of comments for topic modeling: {len(df_clean)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331f0ba",
   "metadata": {},
   "source": [
    "Note that we're removing very short comments less than 20 characters, since these rarely provide any novel topics besides simple sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "eac844c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After preprocessing: 349 documents ready for topic modeling\n",
      "\n",
      "=== ORIGINAL vs PREPROCESSED COMMENTS ===\n",
      "\n",
      "--- Sample 1 ---\n",
      "ORIGINAL: Agent was very helpful from start to finish and made sure we got the right package for us at the best price.\n",
      "PREPROCESSED: agent helpful start finish made sure got right package u best price\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 2 ---\n",
      "ORIGINAL: Quick and efficient and good coverage\n",
      "PREPROCESSED: quick efficient good coverage\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 3 ---\n",
      "ORIGINAL: The person on the phone was polite and gave me all the information I needed to make the right choice of which broadband was the best one for me\n",
      "PREPROCESSED: person phone polite gave information needed make right choice broadband best one\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 4 ---\n",
      "ORIGINAL: Very polite & extremely helpful , great service\n",
      "PREPROCESSED: polite extremely helpful great service\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 5 ---\n",
      "ORIGINAL: Good service much cheaper than sky\n",
      "PREPROCESSED: good service much cheaper sky\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 6 ---\n",
      "ORIGINAL: Communications terrible\n",
      "PREPROCESSED: communication terrible\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 7 ---\n",
      "ORIGINAL: Not happy with way contract was sold to me\n",
      "PREPROCESSED: happy way contract sold\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 8 ---\n",
      "ORIGINAL: Well, I give a 4 because the set up went fairly smooth but the next day I call in the c.s to update my bank details and found out that there is an ongoing broadband issue in my area.\n",
      "PREPROCESSED: well give set went fairly smooth next day call update bank detail found ongoing broadband issue area\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 9 ---\n",
      "ORIGINAL: Despite having entered all my data online I then had to go through everything on the phone It was extremely difficult to understand the advisor\n",
      "PREPROCESSED: despite entered data online go everything phone extremely difficult understand advisor\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 10 ---\n",
      "ORIGINAL: Representative very helpful, gave good advice, understood my needs\n",
      "PREPROCESSED: representative helpful gave good advice understood need\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Use the original text preprocessing approach defined earlier\n",
    "# This uses the clean_text_series function with proper stopword removal and lemmatization\n",
    "\n",
    "# Apply the original preprocessing to prepare documents for BERTopic\n",
    "df_clean['text_processed'] = clean_text_series(\n",
    "    df_clean['LTR_COMMENT'], \n",
    "    return_tokens=False, \n",
    "    min_word_length=2, \n",
    "    language='english'\n",
    ")\n",
    "\n",
    "# Filter out empty processed texts\n",
    "df_clean = df_clean[df_clean['text_processed'].str.len() > 0].copy()\n",
    "\n",
    "# Prepare final documents for topic modeling\n",
    "docs = df_clean['text_processed'].tolist()\n",
    "\n",
    "print(f\"After preprocessing: {len(docs)} documents ready for topic modeling\")\n",
    "\n",
    "# Print comparison of original vs cleaned comments\n",
    "print(\"\\n=== ORIGINAL vs PREPROCESSED COMMENTS ===\\n\")\n",
    "for i, (idx, row) in enumerate(df_clean.sample(n=10, random_state=42).iterrows(), 1):\n",
    "    print(f\"--- Sample {i} ---\")\n",
    "    print(f\"ORIGINAL: {row['LTR_COMMENT']}\")\n",
    "    print(f\"PREPROCESSED: {row['text_processed']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "4739141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize BERTopic model\n",
    "# BERT embeddings and clustering\n",
    "topic_model = BERTopic(\n",
    "    verbose=True,\n",
    "    calculate_probabilities=True,  # Enable probability calculation\n",
    "    nr_topics='auto'  # Let BERTopic determine optimal number of topics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "319796ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 10:48:52,191 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting BERTopic model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cd4cdfa4cf4e3d9f5a574d4b7893f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 10:48:54,313 - BERTopic - Embedding - Completed ✓\n",
      "2025-08-06 10:48:54,314 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-08-06 10:48:54,437 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-08-06 10:48:54,438 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-08-06 10:48:54,455 - BERTopic - Cluster - Completed ✓\n",
      "2025-08-06 10:48:54,455 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-08-06 10:48:54,470 - BERTopic - Representation - Completed ✓\n",
      "2025-08-06 10:48:54,470 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-08-06 10:48:54,474 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-08-06 10:48:54,484 - BERTopic - Representation - Completed ✓\n",
      "2025-08-06 10:48:54,484 - BERTopic - Topic reduction - Reduced number of topics from 9 to 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Fit the model and get topics\n",
    "print(\"Fitting BERTopic model...\")\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# Step 5: Add results back to dataframe\n",
    "df_clean = (df_clean\n",
    "    .assign(\n",
    "        topic=topics,\n",
    "        topic_prob=probs.max(axis=1) if hasattr(probs, 'max') else [max(p) for p in probs]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ef36a51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of topics generated: 9\n",
      "\n",
      "Topic Information:\n",
      "   Topic  Count                                       Name                                     Representation                                Representative_Docs\n",
      "0     -1    116             -1_service_helpful_good_polite  [service, helpful, good, polite, friendly, com...  [called join new customer moving new property ...\n",
      "1      0     76                 0_company_would_call_phone  [company, would, call, phone, broadband, day, ...  [agent set account said would give u 150 free ...\n",
      "2      1     43         1_customer_service_excellent_great  [customer, service, excellent, great, good, pr...  [good customer service, good customer service,...\n",
      "3      2     33          2_helpful_explained_friendly_help  [helpful, explained, friendly, help, everythin...  [friendly explained thing well, helpful explai...\n",
      "4      3     21  3_engineer_professional_hedge_obstruction  [engineer, professional, hedge, obstruction, j...  [experience really good price bit expensive go...\n",
      "5      4     20       4_installation_engineer_came_contact  [installation, engineer, came, contact, instal...  [engineer came installation phenomenal, engine...\n",
      "6      5     14               5_staff_friendly_simple_easy  [staff, friendly, simple, easy, explained, pat...  [easy set process helpful staff phone, simple ...\n",
      "7      6     13         6_quick_service_efficient_coverage  [quick, service, efficient, coverage, friendly...  [quick efficient service easy set, friendly qu...\n",
      "8      7     13     7_communication_problem_daughter_thing  [communication, problem, daughter, thing, good...  [good communication easy solve net problem hel...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 6: Display topic information\n",
    "print(f\"\\nNumber of topics generated: {len(topic_model.get_topic_info())}\")\n",
    "print(\"\\nTopic Information:\")\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "637a9d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample documents by topic:\n",
      "\n",
      "--- Topic 0 ---\n",
      "1. phone helpful fella question got running little time better internet around area...\n",
      "2. set yet speaking ashley informative pleasant saturday chap came day road house infrastructure satisf...\n",
      "3. higher speed sky connection disappointed connected 3x tv tablet mobile phone via wifi rubbish signal...\n",
      "\n",
      "--- Topic 1 ---\n",
      "1. good customer service...\n",
      "2. amazing service job completed...\n",
      "3. hood customer service...\n",
      "\n",
      "--- Topic 2 ---\n",
      "1. far good charlie efficient helpful let hope continues confident...\n",
      "2. friendly easy talk understood wanted happy help...\n",
      "3. explained could understand said thank lady spoke...\n",
      "\n",
      "--- Topic 3 ---\n",
      "1. extremely helpful engineer visit jo 32725...\n",
      "2. mat brilliant followed advailabe really professional plesent deal...\n",
      "3. professional service...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 7: Show representative documents for each topic\n",
    "print(\"\\nSample documents by topic:\")\n",
    "for topic_id in topic_info['Topic'][:5]:  # Show first 5 topics\n",
    "    if topic_id != -1:  # Skip outlier topic\n",
    "        print(f\"\\n--- Topic {topic_id} ---\")\n",
    "        sample_docs = df_clean[df_clean['topic'] == topic_id]['LTR_COMMENT_CLEAN'].head(3)\n",
    "        for i, doc in enumerate(sample_docs, 1):\n",
    "            print(f\"{i}. {doc[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "32dc5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 8: Topic reduction (if needed)\n",
    "# print(f\"\\nOriginal number of topics: {len(topic_model.get_topic_info())}\")\n",
    "\n",
    "# # Reduce topics if there are too many (following guide's example)\n",
    "# if len(topic_model.get_topic_info()) > 20:\n",
    "#     print(\"Reducing number of topics...\")\n",
    "#     topic_model.reduce_topics(docs, nr_topics=15)\n",
    "    \n",
    "#     # Update topics after reduction\n",
    "#     topics_reduced = topic_model.topics_\n",
    "#     df_clean['topic_reduced'] = topics_reduced\n",
    "    \n",
    "#     print(f\"Reduced number of topics: {len(topic_model.get_topic_info())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ac350",
   "metadata": {},
   "source": [
    "**Some rough notes, keeping for documentation**\n",
    "\n",
    "Some things to consider:\n",
    "\n",
    "- We can remove all sentiment related words, does this improve concrete topic extraction?\n",
    "- Substantively, should our topics include sentiment related words? What's a justifiable methodology for our three questions?\n",
    "\n",
    "Should dedicate an entire section to outlining the methodology surrounding stop words. At times, adjectives are useful. And at times, they aren't.\n",
    "\n",
    "- It may be possible to do a continuous classification, rather than a discrete classification of each comment into every category. And then apply a threshold based on cosine similarity etc.\n",
    "\n",
    "Need to look into fine-tuning the BERTopic model or using a different approach, current topics just aren't substantively relevant at the moment. Need to fine-tune based on addressing the specific segmentation questions.\n",
    "\n",
    "Add an LLM-based implementation? May be more adaptive to the nuances of our data.\n",
    "\n",
    "Should also look into entity analysis, which can extract the proper nouns and things within each response, and then pass these onto the topic/theme model. [text](https://cloud.google.com/natural-language/docs/analyzing-entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4f2e9f",
   "metadata": {},
   "source": [
    "### Approach 2: Entity Extraction, Categorical Preprocessing and Filtered Topic Modelling\n",
    "\n",
    "The problem with approach 1, is that while sometimes useful, the topics are too generic. What does helpful_friendly_explained_everything actually mean? They all just boil down to good customer service. \n",
    "\n",
    "We're really interested in questions like:\n",
    "- are competitors mentioned?\n",
    "- do they refer to the customer service staff?\n",
    "- do they refer to timeliness? (being on hold?)\n",
    "- do they refer to contact patterns (getting callbacks etc.)?\n",
    "\n",
    "A better approach is to extract out topics based on certain areas: adjectives, nouns, and perhaps verbs separately. Then rerun our topic analysis.\n",
    "\n",
    "Nouns are most useful for our areas of interest. We'll try that first.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad8d26",
   "metadata": {},
   "source": [
    "#### Noun Extraction with Spacy NLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "6b24fd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy loaded successfully\n",
      "Extracting nouns from customer comments...\n",
      "Using ORIGINAL text (not preprocessed) for better noun recognition\n",
      "Using spaCy for advanced POS tagging\n",
      "Processing comment 1/10...\n",
      "Processing comment 2/10...\n",
      "Processing comment 3/10...\n",
      "Processing comment 4/10...\n",
      "Processing comment 5/10...\n",
      "Processing comment 6/10...\n",
      "Processing comment 7/10...\n",
      "Processing comment 8/10...\n",
      "Processing comment 9/10...\n",
      "Processing comment 10/10...\n",
      "\n",
      "=== NOUN EXTRACTION RESULTS ===\n",
      "\n",
      "--- Sample 1 ---\n",
      "ORIGINAL: Agent was very helpful from start to finish and made sure we got the right package for us at the best price.\n",
      "PREPROCESSED: agent helpful start finish made sure got right package u best price\n",
      "COMMON NOUNS: ['start', 'package', 'price']\n",
      "PROPER NOUNS: ['agent']\n",
      "ALL NOUNS: ['agent', 'package', 'price', 'start']\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 2 ---\n",
      "ORIGINAL: Quick and efficient and good coverage\n",
      "PREPROCESSED: quick efficient good coverage\n",
      "COMMON NOUNS: ['coverage']\n",
      "PROPER NOUNS: []\n",
      "ALL NOUNS: ['coverage']\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 3 ---\n",
      "ORIGINAL: The person on the phone was polite and gave me all the information I needed to make the right choice of which broadband was the best one for me\n",
      "PREPROCESSED: person phone polite gave information needed make right choice broadband best one\n",
      "COMMON NOUNS: ['person', 'phone', 'information', 'choice', 'broadband']\n",
      "PROPER NOUNS: []\n",
      "ALL NOUNS: ['information', 'broadband', 'person', 'phone', 'choice']\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 4 ---\n",
      "ORIGINAL: Very polite & extremely helpful , great service\n",
      "PREPROCESSED: polite extremely helpful great service\n",
      "COMMON NOUNS: ['service']\n",
      "PROPER NOUNS: []\n",
      "ALL NOUNS: ['service']\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 5 ---\n",
      "ORIGINAL: Good service much cheaper than sky\n",
      "PREPROCESSED: good service much cheaper sky\n",
      "COMMON NOUNS: ['service', 'sky']\n",
      "PROPER NOUNS: []\n",
      "ALL NOUNS: ['sky', 'service']\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 6 ---\n",
      "ORIGINAL: Communications terrible\n",
      "PREPROCESSED: communication terrible\n",
      "COMMON NOUNS: ['communication']\n",
      "PROPER NOUNS: []\n",
      "ALL NOUNS: ['communication']\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 7 ---\n",
      "ORIGINAL: Not happy with way contract was sold to me\n",
      "PREPROCESSED: happy way contract sold\n",
      "COMMON NOUNS: ['contract']\n",
      "PROPER NOUNS: []\n",
      "ALL NOUNS: ['contract']\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 8 ---\n",
      "ORIGINAL: Well, I give a 4 because the set up went fairly smooth but the next day I call in the c.s to update my bank details and found out that there is an ongoing broadband issue in my area.\n",
      "PREPROCESSED: well give set went fairly smooth next day call update bank detail found ongoing broadband issue area\n",
      "COMMON NOUNS: ['set', 'bank', 'detail', 'broadband', 'issue', 'area']\n",
      "PROPER NOUNS: ['c.s']\n",
      "ALL NOUNS: ['bank', 'c.s', 'broadband', 'detail', 'area', 'issue', 'set']\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 9 ---\n",
      "ORIGINAL: Despite having entered all my data online I then had to go through everything on the phone It was extremely difficult to understand the advisor\n",
      "PREPROCESSED: despite entered data online go everything phone extremely difficult understand advisor\n",
      "COMMON NOUNS: ['datum', 'phone', 'advisor']\n",
      "PROPER NOUNS: []\n",
      "ALL NOUNS: ['advisor', 'datum', 'phone']\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 10 ---\n",
      "ORIGINAL: Representative very helpful, gave good advice, understood my needs\n",
      "PREPROCESSED: representative helpful gave good advice understood need\n",
      "COMMON NOUNS: ['advice', 'need']\n",
      "PROPER NOUNS: ['representative']\n",
      "ALL NOUNS: ['representative', 'advice', 'need']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== SUMMARY STATISTICS ===\n",
      "Total samples processed: 10\n",
      "Samples with common nouns: 10\n",
      "Samples with proper nouns: 3\n",
      "Samples with any nouns: 10\n",
      "Most common nouns: [('phone', 2), ('broadband', 2), ('service', 2), ('start', 1), ('package', 1), ('price', 1), ('coverage', 1), ('person', 1), ('information', 1), ('choice', 1)]\n",
      "Most common proper nouns: [('agent', 1), ('c.s', 1), ('representative', 1)]\n",
      "Most frequent nouns overall: [('broadband', 2), ('phone', 2), ('service', 2), ('agent', 1), ('package', 1), ('price', 1), ('start', 1), ('coverage', 1), ('information', 1), ('person', 1), ('choice', 1), ('sky', 1), ('communication', 1), ('contract', 1), ('bank', 1)]\n",
      "\n",
      "Method used: spaCy\n"
     ]
    }
   ],
   "source": [
    "# Noun Extraction using spaCy (more reliable than NLTK for POS tagging)\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "# Install spaCy if not already installed\n",
    "# Run this in terminal: pip install spacy\n",
    "# Then download English model: python -m spacy download en_core_web_sm\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    # Load the English language model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy loaded successfully\")\n",
    "    spacy_available = True\n",
    "except ImportError:\n",
    "    print(\"spaCy not installed. Please run:\")\n",
    "    print(\"pip install spacy\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")\n",
    "    spacy_available = False\n",
    "except OSError:\n",
    "    print(\"spaCy English model not found. Please run:\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")\n",
    "    spacy_available = False\n",
    "\n",
    "def extract_nouns_with_spacy(text_content: str):\n",
    "    \"\"\"\n",
    "    Extract nouns using spaCy's advanced POS tagging.\n",
    "    Returns both common nouns and proper nouns.\n",
    "    \"\"\"\n",
    "    if pd.isna(text_content) or not text_content.strip() or not spacy_available:\n",
    "        return {'common_nouns': [], 'proper_nouns': [], 'all_nouns': []}\n",
    "    \n",
    "    try:\n",
    "        # Process the text with spaCy\n",
    "        doc = nlp(text_content)\n",
    "        \n",
    "        common_nouns = []\n",
    "        proper_nouns = []\n",
    "        \n",
    "        for token in doc:\n",
    "            # Skip punctuation, spaces, and very short words\n",
    "            if token.is_punct or token.is_space or len(token.text) < 3:\n",
    "                continue\n",
    "                \n",
    "            # Extract different types of nouns\n",
    "            if token.pos_ == 'NOUN':  # Common nouns\n",
    "                common_nouns.append(token.lemma_.lower())\n",
    "            elif token.pos_ == 'PROPN':  # Proper nouns\n",
    "                proper_nouns.append(token.text.lower())\n",
    "        \n",
    "        # Remove common stopwords that might slip through\n",
    "        stop_words = {'thing', 'time', 'way', 'day', 'people', 'lot', 'bit'}\n",
    "        common_nouns = [noun for noun in common_nouns if noun not in stop_words]\n",
    "        proper_nouns = [noun for noun in proper_nouns if noun not in stop_words]\n",
    "        \n",
    "        # Combine all nouns\n",
    "        all_nouns = list(set(common_nouns + proper_nouns))\n",
    "        \n",
    "        return {\n",
    "            'common_nouns': common_nouns,\n",
    "            'proper_nouns': proper_nouns,\n",
    "            'all_nouns': all_nouns\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text with spaCy: {str(e)[:100]}...\")\n",
    "        return {'common_nouns': [], 'proper_nouns': [], 'all_nouns': []}\n",
    "\n",
    "# Fallback function using NLTK if spaCy is not available\n",
    "def extract_nouns_with_nltk_fallback(text_content: str):\n",
    "    \"\"\"\n",
    "    Fallback noun extraction using NLTK if spaCy is not available.\n",
    "    \"\"\"\n",
    "    if pd.isna(text_content) or not text_content.strip():\n",
    "        return {'common_nouns': [], 'proper_nouns': [], 'all_nouns': []}\n",
    "    \n",
    "    try:\n",
    "        import nltk\n",
    "        from nltk import word_tokenize, pos_tag\n",
    "        \n",
    "        # Ensure required NLTK data is downloaded\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "        \n",
    "        # Tokenize and tag parts of speech\n",
    "        tokens = word_tokenize(text_content)\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        \n",
    "        common_nouns = []\n",
    "        proper_nouns = []\n",
    "        \n",
    "        for word, pos in pos_tags:\n",
    "            if len(word) < 3 or not word.isalpha():\n",
    "                continue\n",
    "                \n",
    "            if pos in ['NN', 'NNS']:  # Common nouns\n",
    "                common_nouns.append(word.lower())\n",
    "            elif pos in ['NNP', 'NNPS']:  # Proper nouns\n",
    "                proper_nouns.append(word.lower())\n",
    "        \n",
    "        # Remove common stopwords\n",
    "        stop_words = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'its', 'may', 'new', 'now', 'old', 'see', 'two', 'who', 'boy', 'did', 'man', 'way', 'bit', 'lot', 'time', 'thing', 'people'}\n",
    "        common_nouns = [noun for noun in common_nouns if noun not in stop_words]\n",
    "        proper_nouns = [noun for noun in proper_nouns if noun not in stop_words]\n",
    "        \n",
    "        all_nouns = list(set(common_nouns + proper_nouns))\n",
    "        \n",
    "        return {\n",
    "            'common_nouns': common_nouns,\n",
    "            'proper_nouns': proper_nouns,\n",
    "            'all_nouns': all_nouns\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in NLTK processing: {e}\")\n",
    "        return {'common_nouns': [], 'proper_nouns': [], 'all_nouns': []}\n",
    "\n",
    "def extract_nouns_from_text(text_content: str):\n",
    "    \"\"\"\n",
    "    Main function to extract nouns. Uses spaCy if available, otherwise NLTK.\n",
    "    \"\"\"\n",
    "    if spacy_available:\n",
    "        return extract_nouns_with_spacy(text_content)\n",
    "    else:\n",
    "        return extract_nouns_with_nltk_fallback(text_content)\n",
    "\n",
    "# Apply noun extraction to a sample of comments\n",
    "print(\"Extracting nouns from customer comments...\")\n",
    "print(\"Using ORIGINAL text (not preprocessed) for better noun recognition\")\n",
    "\n",
    "if spacy_available:\n",
    "    print(\"Using spaCy for advanced POS tagging\")\n",
    "else:\n",
    "    print(\"Using NLTK fallback for POS tagging\")\n",
    "\n",
    "# Take a sample for testing\n",
    "sample_size = 10\n",
    "sample_df = df_clean.sample(n=sample_size, random_state=42).copy()\n",
    "\n",
    "# Extract nouns using ORIGINAL text\n",
    "noun_results = []\n",
    "for idx, row in sample_df.iterrows():\n",
    "    print(f\"Processing comment {len(noun_results) + 1}/{sample_size}...\")\n",
    "    # Use original text for noun extraction, not preprocessed\n",
    "    result = extract_nouns_from_text(row['LTR_COMMENT'])\n",
    "    result['index'] = idx\n",
    "    noun_results.append(result)\n",
    "\n",
    "# Add results back to sample dataframe\n",
    "sample_df['common_nouns'] = [result['common_nouns'] for result in noun_results]\n",
    "sample_df['proper_nouns'] = [result['proper_nouns'] for result in noun_results]\n",
    "sample_df['all_nouns'] = [result['all_nouns'] for result in noun_results]\n",
    "\n",
    "# Display results with comparison\n",
    "print(\"\\n=== NOUN EXTRACTION RESULTS ===\\n\")\n",
    "\n",
    "for i, (idx, row) in enumerate(sample_df.iterrows(), 1):\n",
    "    print(f\"--- Sample {i} ---\")\n",
    "    print(f\"ORIGINAL: {row['LTR_COMMENT']}\")\n",
    "    print(f\"PREPROCESSED: {row['text_processed']}\")\n",
    "    print(f\"COMMON NOUNS: {row['common_nouns']}\")\n",
    "    print(f\"PROPER NOUNS: {row['proper_nouns']}\")\n",
    "    print(f\"ALL NOUNS: {row['all_nouns']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n=== SUMMARY STATISTICS ===\")\n",
    "print(f\"Total samples processed: {len(sample_df)}\")\n",
    "print(f\"Samples with common nouns: {sum(len(nouns) > 0 for nouns in sample_df['common_nouns'])}\")\n",
    "print(f\"Samples with proper nouns: {sum(len(nouns) > 0 for nouns in sample_df['proper_nouns'])}\")\n",
    "print(f\"Samples with any nouns: {sum(len(nouns) > 0 for nouns in sample_df['all_nouns'])}\")\n",
    "\n",
    "# Most common nouns across all categories\n",
    "all_common_nouns = [noun for nouns_list in sample_df['common_nouns'] for noun in nouns_list]\n",
    "all_proper_nouns = [noun for nouns_list in sample_df['proper_nouns'] for noun in nouns_list]\n",
    "all_nouns_combined = [noun for nouns_list in sample_df['all_nouns'] for noun in nouns_list]\n",
    "\n",
    "if all_common_nouns:\n",
    "    common_noun_counts = Counter(all_common_nouns)\n",
    "    print(f\"Most common nouns: {common_noun_counts.most_common(10)}\")\n",
    "\n",
    "if all_proper_nouns:\n",
    "    proper_noun_counts = Counter(all_proper_nouns)\n",
    "    print(f\"Most common proper nouns: {proper_noun_counts.most_common(5)}\")\n",
    "\n",
    "if all_nouns_combined:\n",
    "    all_noun_counts = Counter(all_nouns_combined)\n",
    "    print(f\"Most frequent nouns overall: {all_noun_counts.most_common(15)}\")\n",
    "\n",
    "print(f\"\\nMethod used: {'spaCy' if spacy_available else 'NLTK fallback'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f14f0567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== APPLYING NOUN EXTRACTION TO FULL DATASET ===\n",
      "Extracting nouns from all comments in the dataset...\n",
      "Processing 349 comments...\n",
      "Processed 100/349 comments...\n",
      "Processed 200/349 comments...\n",
      "Processed 300/349 comments...\n",
      "Processed 500/349 comments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 10:48:57,237 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun extraction complete for 349 comments\n",
      "Created 322 noun-only documents for topic modeling\n",
      "Filtered out 27 comments with no extractable nouns\n",
      "\n",
      "=== SAMPLE NOUN-ONLY DOCUMENTS ===\n",
      "--- Sample 1 ---\n",
      "ORIGINAL: Very good customer service\n",
      "NOUN-ONLY: customer service\n",
      "------------------------------------------------------------\n",
      "--- Sample 2 ---\n",
      "ORIGINAL: Because Chris was amazing when she contacted me after I had put my details online.\n",
      "NOUN-ONLY: detail\n",
      "------------------------------------------------------------\n",
      "--- Sample 3 ---\n",
      "ORIGINAL: Price of my bundle went up 2 days after i placed the order\n",
      "NOUN-ONLY: price bundle order\n",
      "------------------------------------------------------------\n",
      "--- Sample 4 ---\n",
      "ORIGINAL: Simple and straightforward process but a lot of information and things to remember over the phone.\n",
      "NOUN-ONLY: process information phone\n",
      "------------------------------------------------------------\n",
      "--- Sample 5 ---\n",
      "ORIGINAL: Amazing service and job completed\n",
      "NOUN-ONLY: service job\n",
      "------------------------------------------------------------\n",
      "--- Sample 6 ---\n",
      "ORIGINAL: The guy I spoke to was very helpful, give me a fantastic deal, and I'm looking forward to receiving my box .. the only downfall is the waiting time of just over two weeks but I'm sure it will be worth the wait.\n",
      "NOUN-ONLY: guy deal box downfall week wait\n",
      "------------------------------------------------------------\n",
      "--- Sample 7 ---\n",
      "ORIGINAL: The work we have had done so far has been done to a high standard. Both the phone agent and the people who came to our house were very professional and polite. Would definitely recommend.\n",
      "NOUN-ONLY: work standard phone agent house\n",
      "------------------------------------------------------------\n",
      "--- Sample 8 ---\n",
      "ORIGINAL: Spoke with several company people. My first contact was with steph who took extreme care to explain all the options available on a conversation that lasted over an hour. Many thanks steph. Following that I spoke with three more people as it became clear to my husband that the penalty fee for leaving BT put us in the invidious position, as pensioners, of not being able to proceed. I spoke with Ine who managed somehow to procure part of BT's despicable fee which finally allowed us to go ahead. Many thanks therefore to the people who helped.\n",
      "NOUN-ONLY: company contact steph care option conversation hour thank steph husband penalty fee position pensioner part fee thank\n",
      "------------------------------------------------------------\n",
      "--- Sample 9 ---\n",
      "ORIGINAL: they where nice and pleasent aswell\n",
      "NOUN-ONLY: aswell\n",
      "------------------------------------------------------------\n",
      "--- Sample 10 ---\n",
      "ORIGINAL: On the phone was a helpful fella all my questions and got me up and running in very little time and better internet around my area\n",
      "NOUN-ONLY: phone fella question internet area\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== RUNNING BERTOPIC ON NOUN-ONLY DOCUMENTS ===\n",
      "Fitting BERTopic model on noun-only documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8efe51316594d979801f8cfa75e3f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 10:48:59,008 - BERTopic - Embedding - Completed ✓\n",
      "2025-08-06 10:48:59,009 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-08-06 10:48:59,128 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-08-06 10:48:59,129 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-08-06 10:48:59,143 - BERTopic - Cluster - Completed ✓\n",
      "2025-08-06 10:48:59,144 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-08-06 10:48:59,152 - BERTopic - Representation - Completed ✓\n",
      "2025-08-06 10:48:59,153 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-08-06 10:48:59,156 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-08-06 10:48:59,162 - BERTopic - Representation - Completed ✓\n",
      "2025-08-06 10:48:59,163 - BERTopic - Topic reduction - Reduced number of topics from 9 to 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of noun-based topics generated: 9\n",
      "\n",
      "Noun-based Topic Information:\n",
      "   Topic  Count                                   Name                                     Representation                                Representative_Docs\n",
      "0     -1    105            -1_company_medium_agent_sky  [company, medium, agent, sky, call, experience...  [phone company medium behalf cousin broadband ...\n",
      "1      0     58     0_company_broadband_phone_property  [company, broadband, phone, property, internet...  [router whole house computer house phone hub r...\n",
      "2      1     40         1_customer_service_fraud_email  [customer, service, fraud, email, company, hel...  [customer service, customer service, customer ...\n",
      "3      2     28           2_staff_process_call_handler  [staff, process, call, handler, item, phone, d...   [service staff, call staff, process staff phone]\n",
      "4      3     23  3_installation_engineer_contact_order  [installation, engineer, contact, order, 3rd, ...  [installation engineer team, agent engineer in...\n",
      "5      4     21          4_problem_help_aswell_enquiry  [problem, help, aswell, enquiry, login, upto, ...          [inquiry problem, login problem, problem]\n",
      "6      5     21      5_service_coverage_future_success  [service, coverage, future, success, dish, pol...                        [service, service, service]\n",
      "7      6     15            6_price_start_value_service  [price, start, value, service, deal, range, co...      [service price, service price, price service]\n",
      "8      7     11             7_lady_guy_gentleman_raise  [lady, guy, gentleman, raise, toplease, deligh...  [lady name delight to.please chat, lady though...\n",
      "\n",
      "Sample documents by noun-based topic:\n",
      "\n",
      "--- Noun Topic 0 ---\n",
      "1. ORIGINAL: On the phone was a helpful fella all my questions and got me up and running in v...\n",
      "   NOUNS: phone fella question internet area\n",
      "2. ORIGINAL: We have not been set up yet, but speaking to Ashley very informative and pleasan...\n",
      "   NOUNS: chap road house infrastructure today site management\n",
      "3. ORIGINAL: Higher speed than SKY.but connection disappointed.We have connected 3x TV,1 tabl...\n",
      "   NOUNS: speed connection tv,1 phone rubbish signal router light good engineer work complaint\n",
      "\n",
      "--- Noun Topic 1 ---\n",
      "1. ORIGINAL: Very good customer service...\n",
      "   NOUNS: customer service\n",
      "2. ORIGINAL: Hood customer service...\n",
      "   NOUNS: customer service\n",
      "3. ORIGINAL: Excellent customer service...\n",
      "   NOUNS: customer service\n",
      "\n",
      "--- Noun Topic 2 ---\n",
      "1. ORIGINAL: Simple and straightforward process but a lot of information and things to rememb...\n",
      "   NOUNS: process information phone\n",
      "2. ORIGINAL: Was promised a call back to change direct debit details but no call received...\n",
      "   NOUNS: call debit detail call\n",
      "3. ORIGINAL: Very efficient, friendly but proffessional experience from initial phone call to...\n",
      "   NOUNS: experience phone call fitting\n",
      "\n",
      "--- Noun Topic 3 ---\n",
      "1. ORIGINAL: Extremely helpful engineer on visit Jo 32725...\n",
      "   NOUNS: engineer\n",
      "2. ORIGINAL: Dee, who took me through the ordering process, could not have been more helpful....\n",
      "   NOUNS: ordering process\n",
      "3. ORIGINAL: Polite, efficient installation thank you. Everything I would expect from company...\n",
      "   NOUNS: installation company\n",
      "\n",
      "=== NOUN-BASED TOPIC MODELING SUMMARY ===\n",
      "Total comments processed: 349\n",
      "Comments with extractable nouns: 322\n",
      "Percentage with nouns: 92.3%\n",
      "Number of topics discovered: 8\n",
      "\n",
      "Most common nouns across all comments: [('service', 136), ('company', 76), ('customer', 71), ('phone', 38), ('call', 34), ('engineer', 27), ('installation', 25), ('medium', 25), ('staff', 25), ('package', 21), ('agent', 19), ('price', 18), ('process', 18), ('contract', 17), ('internet', 16), ('experience', 15), ('team', 15), ('order', 14), ('deal', 14), ('broadband', 14)]\n"
     ]
    }
   ],
   "source": [
    "# Apply noun extraction to the full dataset and run BERTopic on common nouns\n",
    "print(\"=== APPLYING NOUN EXTRACTION TO FULL DATASET ===\")\n",
    "\n",
    "# Extract nouns from all comments in the dataset\n",
    "print(\"Extracting nouns from all comments in the dataset...\")\n",
    "print(f\"Processing {len(df_clean)} comments...\")\n",
    "\n",
    "# Note: We extract nouns from ORIGINAL text, not preprocessed text\n",
    "# This preserves proper nouns and context for better POS tagging\n",
    "all_noun_results = []\n",
    "for idx, row in df_clean.iterrows():\n",
    "    if idx % 100 == 0:  # Progress indicator\n",
    "        print(f\"Processed {idx}/{len(df_clean)} comments...\")\n",
    "    \n",
    "    # Extract nouns from ORIGINAL text (not preprocessed)\n",
    "    result = extract_nouns_from_text(row['LTR_COMMENT'])\n",
    "    all_noun_results.append(result)\n",
    "\n",
    "# Add noun extraction results to the full dataframe\n",
    "df_clean['common_nouns'] = [result['common_nouns'] for result in all_noun_results]\n",
    "df_clean['proper_nouns'] = [result['proper_nouns'] for result in all_noun_results]\n",
    "df_clean['all_nouns'] = [result['all_nouns'] for result in all_noun_results]\n",
    "\n",
    "print(f\"Noun extraction complete for {len(df_clean)} comments\")\n",
    "\n",
    "# Create documents from common nouns for BERTopic\n",
    "# Join common nouns with spaces to create noun-only documents\n",
    "noun_docs = []\n",
    "for nouns_list in df_clean['common_nouns']:\n",
    "    if nouns_list:  # Only include if there are nouns\n",
    "        noun_docs.append(' '.join(nouns_list))\n",
    "    else:\n",
    "        noun_docs.append('')  # Empty string for comments with no nouns\n",
    "\n",
    "# Filter out empty noun documents\n",
    "non_empty_indices = [i for i, doc in enumerate(noun_docs) if doc.strip()]\n",
    "filtered_noun_docs = [noun_docs[i] for i in non_empty_indices]\n",
    "filtered_df = df_clean.iloc[non_empty_indices].copy()\n",
    "\n",
    "print(f\"Created {len(filtered_noun_docs)} noun-only documents for topic modeling\")\n",
    "print(f\"Filtered out {len(df_clean) - len(filtered_noun_docs)} comments with no extractable nouns\")\n",
    "\n",
    "# Sample of noun-only documents\n",
    "print(\"\\n=== SAMPLE NOUN-ONLY DOCUMENTS ===\")\n",
    "for i, doc in enumerate(filtered_noun_docs[:10]):\n",
    "    original_comment = filtered_df.iloc[i]['LTR_COMMENT']\n",
    "    print(f\"--- Sample {i+1} ---\")\n",
    "    print(f\"ORIGINAL: {original_comment}\")\n",
    "    print(f\"NOUN-ONLY: {doc}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Initialize BERTopic model for noun-based topic modeling\n",
    "print(\"\\n=== RUNNING BERTOPIC ON NOUN-ONLY DOCUMENTS ===\")\n",
    "noun_topic_model = BERTopic(\n",
    "    verbose=True,\n",
    "    calculate_probabilities=True,\n",
    "    nr_topics='auto'  # Let BERTopic determine optimal number of topics\n",
    ")\n",
    "\n",
    "# Fit the model on noun-only documents\n",
    "print(\"Fitting BERTopic model on noun-only documents...\")\n",
    "noun_topics, noun_probs = noun_topic_model.fit_transform(filtered_noun_docs)\n",
    "\n",
    "# Add results back to filtered dataframe\n",
    "filtered_df = filtered_df.assign(\n",
    "    noun_topic=noun_topics,\n",
    "    noun_topic_prob=noun_probs.max(axis=1) if hasattr(noun_probs, 'max') else [max(p) for p in noun_probs]\n",
    ")\n",
    "\n",
    "# Display topic information\n",
    "print(f\"\\nNumber of noun-based topics generated: {len(noun_topic_model.get_topic_info())}\")\n",
    "print(\"\\nNoun-based Topic Information:\")\n",
    "noun_topic_info = noun_topic_model.get_topic_info()\n",
    "print(noun_topic_info.head(10))\n",
    "\n",
    "# Show representative documents for each topic\n",
    "print(\"\\nSample documents by noun-based topic:\")\n",
    "for topic_id in noun_topic_info['Topic'][:5]:  # Show first 5 topics\n",
    "    if topic_id != -1:  # Skip outlier topic\n",
    "        print(f\"\\n--- Noun Topic {topic_id} ---\")\n",
    "        # Show both original comments and noun-only versions\n",
    "        topic_samples = filtered_df[filtered_df['noun_topic'] == topic_id].head(3)\n",
    "        for i, (idx, row) in enumerate(topic_samples.iterrows(), 1):\n",
    "            print(f\"{i}. ORIGINAL: {row['LTR_COMMENT'][:80]}...\")\n",
    "            print(f\"   NOUNS: {' '.join(row['common_nouns'])}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n=== NOUN-BASED TOPIC MODELING SUMMARY ===\")\n",
    "print(f\"Total comments processed: {len(df_clean)}\")\n",
    "print(f\"Comments with extractable nouns: {len(filtered_df)}\")\n",
    "print(f\"Percentage with nouns: {len(filtered_df)/len(df_clean)*100:.1f}%\")\n",
    "print(f\"Number of topics discovered: {len(noun_topic_info) - 1}\")  # Exclude outlier topic\n",
    "\n",
    "# Most common nouns overall\n",
    "all_common_nouns_full = [noun for nouns_list in df_clean['common_nouns'] for noun in nouns_list]\n",
    "if all_common_nouns_full:\n",
    "    common_noun_counts_full = Counter(all_common_nouns_full)\n",
    "    print(f\"\\nMost common nouns across all comments: {common_noun_counts_full.most_common(20)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2811f732",
   "metadata": {},
   "source": [
    "We're getting somewhere, these seem more useful than before. However even with noun extraction, our topics still feel too generic.\n",
    "\n",
    "### Approach 2: Creating Domain-Specific Topic Categories\n",
    "\n",
    "Instead of purely unsupervised topic modelling, let's implement a hybrid approach that targets our research questions.\n",
    "\n",
    "This uses a keyword encoding scheme to assign points to informed categories from our qualitative analysis and assigns the category with the highest points. It utilizes a threshold so not every comment is automatically classified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d9116e",
   "metadata": {},
   "source": [
    "After significant experimentation and testing. This involved seeing what transformation of specific words create meaningful and useful categorizations for our RQs without generating 1) too many categories with no observations or 2) too few generic categories that don't tell us anything useful. We end up with the following coding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "47dcd0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CATEGORIZING WITH CONSOLIDATED CATEGORIES (FIXED) ===\n",
      "=== CATEGORIES DEFINED IN FUNCTION ===\n",
      "Available categories: ['agent_personality', 'agent_mentions', 'communication_support', 'clarity_communication', 'support_actions', 'process_experience', 'technical_service', 'financial_products', 'timeliness', 'competitive_retention', 'contact_patterns']\n",
      "\n",
      "=== FINAL PRIMARY ASPECT COUNTS (CONSOLIDATED) ===\n",
      "agent_mentions           84\n",
      "unclassified             66\n",
      "technical_service        53\n",
      "financial_products       38\n",
      "process_experience       29\n",
      "agent_personality        26\n",
      "support_actions           9\n",
      "contact_patterns          8\n",
      "communication_support     5\n",
      "competitive_retention     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== PERCENTAGE DISTRIBUTION ===\n",
      "agent_mentions: 26.1%\n",
      "unclassified: 20.5%\n",
      "technical_service: 16.5%\n",
      "financial_products: 11.8%\n",
      "process_experience: 9.0%\n",
      "agent_personality: 8.1%\n",
      "support_actions: 2.8%\n",
      "contact_patterns: 2.5%\n",
      "communication_support: 1.6%\n",
      "competitive_retention: 1.2%\n"
     ]
    }
   ],
   "source": [
    "# COMPLETELY REPLACE the categorize_service_aspects_final function with this:\n",
    "\n",
    "def categorize_service_aspects_final(nouns, comment_text, min_threshold=2):\n",
    "    \"\"\"\n",
    "    Final optimized categorization with consolidated categories - ONLY 7 CATEGORIES\n",
    "    \"\"\"\n",
    "    aspect_keywords = {\n",
    "        # === HUMAN INTERACTION (RQ1 & RQ2) ===\n",
    "        'agent_personality': [\n",
    "            # Positive traits\n",
    "            'professional', 'friendly', 'knowledgeable', 'courteous', 'polite', 'helpful', \n",
    "            'patient', 'understanding', 'excellent', 'brilliant', 'fantastic', 'amazing', \n",
    "            'lovely', 'nice', 'kind', 'pleasant', 'competent', 'supportive', 'attentive',\n",
    "            # Negative traits  \n",
    "            'rude', 'unhelpful', 'impatient', 'arrogant', 'unprofessional', 'terrible', \n",
    "            'awful', 'useless', 'incompetent', 'annoying', 'frustrating', 'unfriendly', \n",
    "            'unapproachable', 'condescending', 'ignorant', 'abrupt', 'argumentative'\n",
    "        ],\n",
    "        \n",
    "        # ADD BACK: Generic agent mentions (captures human element focus)\n",
    "        'agent_mentions': [\n",
    "            'agent', 'staff', 'representative', 'advisor', 'person', 'team', 'operator', \n",
    "            'consultant', 'employee', 'member', 'caller', 'handler', 'lady', 'gentleman', \n",
    "            'guy', 'worker', 'man', 'woman', 'individual', 'member of staff'\n",
    "        ],\n",
    "        \n",
    "        # === COMMUNICATION & SUPPORT (RQ2 & RQ3) ===\n",
    "        'communication_support': [\n",
    "            # Communication quality (merge clarity + information provision)\n",
    "            'explained', 'clear', 'clarified', 'detailed', 'thorough', 'understood', 'made sense',\n",
    "            'simple', 'easy to follow', 'broke down', 'walked through', 'step by step',\n",
    "            'confusing', 'unclear', 'vague', 'didn\\'t understand', 'complicated', 'lost',\n",
    "            'information', 'details', 'options', 'choices', 'alternatives', 'told', 'showed',\n",
    "            'informed', 'outlined', 'went over', 'covered', 'discussed', 'presented'\n",
    "        ],\n",
    "        \n",
    "        # ADD BACK: Specific clarity focus (RQ2 - script effectiveness)\n",
    "        'clarity_communication': [\n",
    "            'clear explanation', 'easy to understand', 'made it clear', 'clearly explained',\n",
    "            'broke it down', 'step by step', 'walked me through', 'made sense', 'understood',\n",
    "            'unclear', 'confusing', 'didn\\'t explain clearly', 'hard to understand', 'vague'\n",
    "        ],\n",
    "        \n",
    "        # ADD BACK: Specific support actions (captures concrete help)\n",
    "        'support_actions': [\n",
    "            'help', 'assistance', 'support', 'guidance', 'advice', 'sorted', 'resolved',\n",
    "            'fixed', 'arranged', 'organized', 'handled', 'dealt with', 'took care',\n",
    "            'sorted out', 'looked into', 'followed up', 'chased up', 'actioned'\n",
    "        ],\n",
    "        \n",
    "        # === PROCESS EXPERIENCE (RQ3) ===\n",
    "        'process_experience': [\n",
    "            # Time efficiency (merge time_efficiency + time_problems)\n",
    "            'quick', 'fast', 'prompt', 'speedy', 'efficient', 'straight away',\n",
    "            'immediately', 'rapid', 'swift', 'instant', 'no time', 'right away',\n",
    "            'waiting', 'hold', 'delay', 'slow', 'ages', 'forever', 'hung up',\n",
    "            'long wait', 'hours', 'kept waiting', 'on hold', 'time wasted',\n",
    "            # Process ease/friction\n",
    "            'easy', 'straightforward', 'simple', 'smooth', 'seamless', 'hassle-free',\n",
    "            'difficult', 'complicated', 'hassle', 'frustrating', 'problem', 'issue',\n",
    "            'nightmare', 'ordeal', 'pain', 'struggle', 'mess', 'disaster'\n",
    "        ],\n",
    "        \n",
    "        # === SERVICE CONTENT (Context - keep as-is) ===\n",
    "        'technical_service': [\n",
    "            'broadband', 'internet', 'wifi', 'connection', 'speed', 'installation', \n",
    "            'engineer', 'technical', 'router', 'modem', 'fiber', 'cable', 'network', 'signal'\n",
    "        ],\n",
    "        \n",
    "        'financial_products': [  # Merge billing + packages\n",
    "            'bill', 'price', 'cost', 'charge', 'payment', 'money', 'expensive', 'value',\n",
    "            'package', 'deal', 'contract', 'plan', 'offer', 'upgrade', 'renewal', 'discount'\n",
    "        ],\n",
    "        \n",
    "        'timeliness': [\n",
    "            'waiting', 'hold', 'delay', 'slow', 'ages', 'forever', 'hung up',\n",
    "            'long wait', 'hours', 'kept waiting', 'on hold', 'time wasted',\n",
    "            'wait time', 'queue', 'holding', 'delayed'\n",
    "        ],\n",
    "        \n",
    "        # === COMPETITIVE CONTEXT (RQ1) ===\n",
    "        'competitive_retention': [  # Merge competitors + retention\n",
    "            'sky', 'bt', 'virgin', 'talk talk', 'plusnet', 'competitor', 'switching',\n",
    "            'stay', 'leave', 'cancel', 'better deal', 'match', 'compete', 'loyalty'\n",
    "        ],\n",
    "        \n",
    "        # === CONTACT PATTERNS (Service Quality Indicator) ===\n",
    "        'contact_patterns': [\n",
    "            'callback', 'call back', 'promised', 'follow up', 'contact', 'ring',\n",
    "            'again', 'second time', 'third call', 'repeatedly', 'still', 'another'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Enhanced scoring logic\n",
    "    aspect_scores = {}\n",
    "    comment_lower = comment_text.lower()\n",
    "    \n",
    "    for aspect, keywords in aspect_keywords.items():\n",
    "        score = 0\n",
    "        \n",
    "        # Noun matches (weight 2)\n",
    "        for noun in nouns:\n",
    "            for keyword in keywords:\n",
    "                if keyword in noun.lower():\n",
    "                    score += 2\n",
    "                    break\n",
    "        \n",
    "        # Context matches with phrase bonuses\n",
    "        for keyword in keywords:\n",
    "            if keyword in comment_lower:\n",
    "                if ' ' in keyword:\n",
    "                    score += 2  # Multi-word phrases get extra weight\n",
    "                else:\n",
    "                    score += 1\n",
    "        \n",
    "        aspect_scores[aspect] = score\n",
    "    \n",
    "    # Apply threshold\n",
    "    max_score = max(aspect_scores.values()) if aspect_scores.values() else 0\n",
    "    \n",
    "    if max_score < min_threshold:\n",
    "        return aspect_scores, 'unclassified'\n",
    "    else:\n",
    "        primary_aspect = max(aspect_scores, key=aspect_scores.get)\n",
    "        return aspect_scores, primary_aspect\n",
    "\n",
    "# Re-run categorization - this should now give ONLY 7 categories + unclassified\n",
    "print(\"=== CATEGORIZING WITH CONSOLIDATED CATEGORIES (FIXED) ===\")\n",
    "aspect_results = []\n",
    "for idx, row in filtered_df.iterrows():\n",
    "    scores, primary_aspect = categorize_service_aspects_final(row['common_nouns'], row['LTR_COMMENT'])\n",
    "    aspect_results.append({'scores': scores, 'primary': primary_aspect})\n",
    "\n",
    "# Extract primary aspects directly from function results\n",
    "primary_aspects = [result['primary'] for result in aspect_results]\n",
    "\n",
    "# Add to dataframe\n",
    "filtered_df['primary_aspect_consolidated'] = primary_aspects\n",
    "\n",
    "# Print categories defined in the function\n",
    "print(\"=== CATEGORIES DEFINED IN FUNCTION ===\")\n",
    "sample_scores = aspect_results[0]['scores']\n",
    "print(\"Available categories:\", list(sample_scores.keys()))\n",
    "\n",
    "# Print final value counts from the actual function results\n",
    "print(\"\\n=== FINAL PRIMARY ASPECT COUNTS (CONSOLIDATED) ===\")\n",
    "aspect_counts = pd.Series(primary_aspects).value_counts()\n",
    "print(aspect_counts)\n",
    "\n",
    "# Show percentage distribution\n",
    "print(\"\\n=== PERCENTAGE DISTRIBUTION ===\")\n",
    "aspect_percentages = pd.Series(primary_aspects).value_counts(normalize=True) * 100\n",
    "for category, percentage in aspect_percentages.items():\n",
    "    print(f\"{category}: {percentage:.1f}%\")\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "141262fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CATEGORIZING WITH CONSOLIDATED CATEGORIES (FIXED) ===\n",
      "=== CATEGORIES DEFINED IN FUNCTION ===\n",
      "Available categories: ['agent_personality', 'agent_mentions', 'communication_support', 'clarity_communication', 'support_actions', 'process_experience', 'technical_service', 'financial_products', 'timeliness', 'competitive_retention', 'contact_patterns']\n",
      "Total categories defined: 11\n",
      "\n",
      "=== COMPLETE PRIMARY ASPECT COUNTS (ALL CATEGORIES) ===\n",
      "agent_mentions: 84\n",
      "unclassified: 66\n",
      "technical_service: 53\n",
      "financial_products: 38\n",
      "process_experience: 29\n",
      "agent_personality: 26\n",
      "support_actions: 9\n",
      "contact_patterns: 8\n",
      "communication_support: 5\n",
      "competitive_retention: 4\n",
      "clarity_communication: 0\n",
      "timeliness: 0\n",
      "\n",
      "Total comments processed: 322\n",
      "\n",
      "=== PERCENTAGE DISTRIBUTION (NON-ZERO CATEGORIES) ===\n",
      "agent_mentions: 84 (26.1%)\n",
      "unclassified: 66 (20.5%)\n",
      "technical_service: 53 (16.5%)\n",
      "financial_products: 38 (11.8%)\n",
      "process_experience: 29 (9.0%)\n",
      "agent_personality: 26 (8.1%)\n",
      "support_actions: 9 (2.8%)\n",
      "contact_patterns: 8 (2.5%)\n",
      "communication_support: 5 (1.6%)\n",
      "competitive_retention: 4 (1.2%)\n",
      "\n",
      "=== SUMMARY ===\n",
      "Categories with comments: 10\n",
      "Categories with zero comments: 2\n",
      "Total categories defined: 12\n",
      "\n",
      "============================================================\n",
      "=== RAW SCORE DISTRIBUTION ANALYSIS ===\n",
      "============================================================\n",
      "\n",
      "=== TOTAL SCORES PER CATEGORY (Sum across all comments) ===\n",
      "agent_mentions: 426\n",
      "technical_service: 378\n",
      "financial_products: 312\n",
      "agent_personality: 284\n",
      "process_experience: 217\n",
      "support_actions: 151\n",
      "communication_support: 98\n",
      "contact_patterns: 71\n",
      "competitive_retention: 60\n",
      "timeliness: 31\n",
      "clarity_communication: 10\n",
      "\n",
      "=== AVERAGE SCORES PER CATEGORY ===\n",
      "agent_mentions: 1.32\n",
      "technical_service: 1.17\n",
      "financial_products: 0.97\n",
      "agent_personality: 0.88\n",
      "process_experience: 0.67\n",
      "support_actions: 0.47\n",
      "communication_support: 0.30\n",
      "contact_patterns: 0.22\n",
      "competitive_retention: 0.19\n",
      "timeliness: 0.10\n",
      "clarity_communication: 0.03\n",
      "\n",
      "=== COMMENTS WITH NON-ZERO SCORES PER CATEGORY ===\n",
      "agent_personality: 176 comments (54.7%)\n",
      "agent_mentions: 132 comments (41.0%)\n",
      "support_actions: 105 comments (32.6%)\n",
      "process_experience: 101 comments (31.4%)\n",
      "technical_service: 94 comments (29.2%)\n",
      "financial_products: 81 comments (25.2%)\n",
      "communication_support: 67 comments (20.8%)\n",
      "contact_patterns: 42 comments (13.0%)\n",
      "competitive_retention: 33 comments (10.2%)\n",
      "timeliness: 17 comments (5.3%)\n",
      "clarity_communication: 7 comments (2.2%)\n",
      "\n",
      "=== SCORE DISTRIBUTION STATISTICS ===\n",
      "Category | Min | Max | Mean | Std | 75th | 90th | 95th\n",
      "------------------------------------------------------------\n",
      "agent_personali |   0 |   7 |  0.9 |  1.1 |  1.0 |  2.0 |  3.0\n",
      "agent_mentions  |   0 |  20 |  1.3 |  2.1 |  3.0 |  3.0 |  4.0\n",
      "communication_s |   0 |   4 |  0.3 |  0.7 |  0.0 |  1.0 |  2.0\n",
      "clarity_communi |   0 |   2 |  0.0 |  0.2 |  0.0 |  0.0 |  0.0\n",
      "support_actions |   0 |   6 |  0.5 |  0.8 |  1.0 |  1.0 |  2.0\n",
      "process_experie |   0 |   8 |  0.7 |  1.3 |  1.0 |  2.0 |  4.0\n",
      "technical_servi |   0 |  16 |  1.2 |  2.3 |  3.0 |  3.0 |  6.0\n",
      "financial_produ |   0 |  15 |  1.0 |  2.1 |  0.8 |  3.0 |  4.9\n",
      "timeliness      |   0 |   6 |  0.1 |  0.5 |  0.0 |  0.0 |  0.9\n",
      "competitive_ret |   0 |   5 |  0.2 |  0.7 |  0.0 |  0.9 |  1.0\n",
      "contact_pattern |   0 |   5 |  0.2 |  0.7 |  0.0 |  1.0 |  2.0\n",
      "\n",
      "=== HIGH-SCORING COMMENTS (Score >= 5) ===\n",
      "\n",
      "agent_personality: 4 comments with score >= 5\n",
      "  Max score: 7\n",
      "  Score distribution: {5: 2, 7: 2}\n",
      "\n",
      "agent_mentions: 15 comments with score >= 5\n",
      "  Max score: 20\n",
      "  Score distribution: {5: 5, 6: 6, 7: 1, 9: 1, 14: 1, 20: 1}\n",
      "\n",
      "support_actions: 1 comments with score >= 5\n",
      "  Max score: 6\n",
      "  Score distribution: {6: 1}\n",
      "\n",
      "process_experience: 7 comments with score >= 5\n",
      "  Max score: 8\n",
      "  Score distribution: {5: 2, 6: 1, 7: 3, 8: 1}\n",
      "\n",
      "technical_service: 24 comments with score >= 5\n",
      "  Max score: 16\n",
      "  Score distribution: {5: 2, 6: 14, 7: 1, 8: 1, 9: 2, 10: 1, 11: 1, 13: 1, 16: 1}\n",
      "\n",
      "financial_products: 17 comments with score >= 5\n",
      "  Max score: 15\n",
      "  Score distribution: {5: 1, 6: 10, 7: 1, 8: 1, 9: 1, 10: 1, 14: 1, 15: 1}\n",
      "\n",
      "timeliness: 1 comments with score >= 5\n",
      "  Max score: 6\n",
      "  Score distribution: {6: 1}\n",
      "\n",
      "competitive_retention: 1 comments with score >= 5\n",
      "  Max score: 5\n",
      "  Score distribution: {5: 1}\n",
      "\n",
      "contact_patterns: 1 comments with score >= 5\n",
      "  Max score: 5\n",
      "  Score distribution: {5: 1}\n",
      "\n",
      "=== MULTI-CATEGORY OVERLAP ANALYSIS ===\n",
      "Comments hitting multiple categories:\n",
      "  0 categories: 21 comments (6.5%)\n",
      "  1 categories: 63 comments (19.6%)\n",
      "  2 categories: 79 comments (24.5%)\n",
      "  3 categories: 73 comments (22.7%)\n",
      "  4 categories: 44 comments (13.7%)\n",
      "  5 categories: 22 comments (6.8%)\n",
      "  6 categories: 13 comments (4.0%)\n",
      "  7 categories: 5 comments (1.6%)\n",
      "  8 categories: 2 comments (0.6%)\n",
      "\n",
      "=== MOST COMMON CATEGORY COMBINATIONS (for multi-category comments) ===\n",
      "Top 10 category combinations:\n",
      "  agent_mentions + agent_personality: 14 (5.9%)\n",
      "  agent_mentions + agent_personality + support_actions: 11 (4.6%)\n",
      "  agent_personality + support_actions: 10 (4.2%)\n",
      "  agent_personality + support_actions + technical_service: 7 (2.9%)\n",
      "  agent_personality + process_experience: 7 (2.9%)\n",
      "  agent_personality + process_experience + support_actions: 5 (2.1%)\n",
      "  agent_mentions + process_experience: 5 (2.1%)\n",
      "  agent_mentions + support_actions: 5 (2.1%)\n",
      "  agent_personality + technical_service: 5 (2.1%)\n",
      "  agent_mentions + agent_personality + technical_service: 5 (2.1%)\n",
      "\n",
      "=== THRESHOLD SENSITIVITY ANALYSIS ===\n",
      "Threshold | Unclassified | Classified | % Classified\n",
      "--------------------------------------------------\n",
      "    1     |     21      |     301     | 93.5%\n",
      "    2     |     66      |     256     | 79.5%\n",
      "    3     |     85      |     237     | 73.6%\n",
      "    4     |     236      |     86     | 26.7%\n",
      "    5     |     266      |     56     | 17.4%\n"
     ]
    }
   ],
   "source": [
    "# COMPLETELY REPLACE the categorize_service_aspects_final function with this:\n",
    "\n",
    "def categorize_service_aspects_final(nouns, comment_text, min_threshold=2):\n",
    "    \"\"\"\n",
    "    Final optimized categorization with consolidated categories - ONLY 7 CATEGORIES\n",
    "    \"\"\"\n",
    "    aspect_keywords = {\n",
    "        # === HUMAN INTERACTION (RQ1 & RQ2) ===\n",
    "        'agent_personality': [\n",
    "            # Positive traits\n",
    "            'professional', 'friendly', 'knowledgeable', 'courteous', 'polite', 'helpful', \n",
    "            'patient', 'understanding', 'excellent', 'brilliant', 'fantastic', 'amazing', \n",
    "            'lovely', 'nice', 'kind', 'pleasant', 'competent', 'supportive', 'attentive',\n",
    "            # Negative traits  \n",
    "            'rude', 'unhelpful', 'impatient', 'arrogant', 'unprofessional', 'terrible', \n",
    "            'awful', 'useless', 'incompetent', 'annoying', 'frustrating', 'unfriendly', \n",
    "            'unapproachable', 'condescending', 'ignorant', 'abrupt', 'argumentative'\n",
    "        ],\n",
    "        \n",
    "        # ADD BACK: Generic agent mentions (captures human element focus)\n",
    "        'agent_mentions': [\n",
    "            'agent', 'staff', 'representative', 'advisor', 'person', 'team', 'operator', \n",
    "            'consultant', 'employee', 'member', 'caller', 'handler', 'lady', 'gentleman', \n",
    "            'guy', 'worker', 'man', 'woman', 'individual', 'member of staff'\n",
    "        ],\n",
    "        \n",
    "        # === COMMUNICATION & SUPPORT (RQ2 & RQ3) ===\n",
    "        'communication_support': [\n",
    "            # Communication quality (merge clarity + information provision)\n",
    "            'explained', 'clear', 'clarified', 'detailed', 'thorough', 'understood', 'made sense',\n",
    "            'simple', 'easy to follow', 'broke down', 'walked through', 'step by step',\n",
    "            'confusing', 'unclear', 'vague', 'didn\\'t understand', 'complicated', 'lost',\n",
    "            'information', 'details', 'options', 'choices', 'alternatives', 'told', 'showed',\n",
    "            'informed', 'outlined', 'went over', 'covered', 'discussed', 'presented'\n",
    "        ],\n",
    "        \n",
    "        # ADD BACK: Specific clarity focus (RQ2 - script effectiveness)\n",
    "        'clarity_communication': [\n",
    "            'clear explanation', 'easy to understand', 'made it clear', 'clearly explained',\n",
    "            'broke it down', 'step by step', 'walked me through', 'made sense', 'understood',\n",
    "            'unclear', 'confusing', 'didn\\'t explain clearly', 'hard to understand', 'vague'\n",
    "        ],\n",
    "        \n",
    "        # ADD BACK: Specific support actions (captures concrete help)\n",
    "        'support_actions': [\n",
    "            'help', 'assistance', 'support', 'guidance', 'advice', 'sorted', 'resolved',\n",
    "            'fixed', 'arranged', 'organized', 'handled', 'dealt with', 'took care',\n",
    "            'sorted out', 'looked into', 'followed up', 'chased up', 'actioned'\n",
    "        ],\n",
    "        \n",
    "        # === PROCESS EXPERIENCE (RQ3) ===\n",
    "        'process_experience': [\n",
    "            # Time efficiency (merge time_efficiency + time_problems)\n",
    "            'quick', 'fast', 'prompt', 'speedy', 'efficient', 'straight away',\n",
    "            'immediately', 'rapid', 'swift', 'instant', 'no time', 'right away',\n",
    "            'waiting', 'hold', 'delay', 'slow', 'ages', 'forever', 'hung up',\n",
    "            'long wait', 'hours', 'kept waiting', 'on hold', 'time wasted',\n",
    "            # Process ease/friction\n",
    "            'easy', 'straightforward', 'simple', 'smooth', 'seamless', 'hassle-free',\n",
    "            'difficult', 'complicated', 'hassle', 'frustrating', 'problem', 'issue',\n",
    "            'nightmare', 'ordeal', 'pain', 'struggle', 'mess', 'disaster'\n",
    "        ],\n",
    "        \n",
    "        # === SERVICE CONTENT (Context - keep as-is) ===\n",
    "        'technical_service': [\n",
    "            'broadband', 'internet', 'wifi', 'connection', 'speed', 'installation', \n",
    "            'engineer', 'technical', 'router', 'modem', 'fiber', 'cable', 'network', 'signal'\n",
    "        ],\n",
    "        \n",
    "        'financial_products': [  # Merge billing + packages\n",
    "            'bill', 'price', 'cost', 'charge', 'payment', 'money', 'expensive', 'value',\n",
    "            'package', 'deal', 'contract', 'plan', 'offer', 'upgrade', 'renewal', 'discount'\n",
    "        ],\n",
    "        \n",
    "        'timeliness': [\n",
    "            'waiting', 'hold', 'delay', 'slow', 'ages', 'forever', 'hung up',\n",
    "            'long wait', 'hours', 'kept waiting', 'on hold', 'time wasted',\n",
    "            'wait time', 'queue', 'holding', 'delayed'\n",
    "        ],\n",
    "        \n",
    "        # === COMPETITIVE CONTEXT (RQ1) ===\n",
    "        'competitive_retention': [  # Merge competitors + retention\n",
    "            'sky', 'bt', 'virgin', 'talk talk', 'plusnet', 'competitor', 'switching',\n",
    "            'stay', 'leave', 'cancel', 'better deal', 'match', 'compete', 'loyalty'\n",
    "        ],\n",
    "        \n",
    "        # === CONTACT PATTERNS (Service Quality Indicator) ===\n",
    "        'contact_patterns': [\n",
    "            'callback', 'call back', 'promised', 'follow up', 'contact', 'ring',\n",
    "            'again', 'second time', 'third call', 'repeatedly', 'still', 'another'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Enhanced scoring logic\n",
    "    aspect_scores = {}\n",
    "    comment_lower = comment_text.lower()\n",
    "    \n",
    "    for aspect, keywords in aspect_keywords.items():\n",
    "        score = 0\n",
    "        \n",
    "        # Noun matches (weight 2)\n",
    "        for noun in nouns:\n",
    "            for keyword in keywords:\n",
    "                if keyword in noun.lower():\n",
    "                    score += 2\n",
    "                    break\n",
    "        \n",
    "        # Context matches with phrase bonuses\n",
    "        for keyword in keywords:\n",
    "            if keyword in comment_lower:\n",
    "                if ' ' in keyword:\n",
    "                    score += 2  # Multi-word phrases get extra weight\n",
    "                else:\n",
    "                    score += 1\n",
    "        \n",
    "        aspect_scores[aspect] = score\n",
    "    \n",
    "    # Apply threshold\n",
    "    max_score = max(aspect_scores.values()) if aspect_scores.values() else 0\n",
    "    \n",
    "    if max_score < min_threshold:\n",
    "        return aspect_scores, 'unclassified'\n",
    "    else:\n",
    "        primary_aspect = max(aspect_scores, key=aspect_scores.get)\n",
    "        return aspect_scores, primary_aspect\n",
    "\n",
    "# Re-run categorization - this should now give ONLY 7 categories + unclassified\n",
    "print(\"=== CATEGORIZING WITH CONSOLIDATED CATEGORIES (FIXED) ===\")\n",
    "aspect_results = []\n",
    "for idx, row in filtered_df.iterrows():\n",
    "    scores, primary_aspect = categorize_service_aspects_final(row['common_nouns'], row['LTR_COMMENT'])\n",
    "    aspect_results.append({'scores': scores, 'primary': primary_aspect})\n",
    "\n",
    "# Extract primary aspects directly from function results\n",
    "primary_aspects = [result['primary'] for result in aspect_results]\n",
    "\n",
    "# Add to dataframe\n",
    "filtered_df['primary_aspect_consolidated'] = primary_aspects\n",
    "\n",
    "# Get all defined categories from the function\n",
    "def get_all_categories():\n",
    "    \"\"\"Extract all category names from the function\"\"\"\n",
    "    return [\n",
    "        'agent_personality', 'agent_mentions', 'communication_support', \n",
    "        'clarity_communication', 'support_actions', 'process_experience',\n",
    "        'technical_service', 'financial_products', 'timeliness', \n",
    "        'competitive_retention', 'contact_patterns'\n",
    "    ]\n",
    "\n",
    "all_categories = get_all_categories()\n",
    "\n",
    "print(\"=== CATEGORIES DEFINED IN FUNCTION ===\")\n",
    "print(\"Available categories:\", all_categories)\n",
    "print(\"Total categories defined:\", len(all_categories))\n",
    "\n",
    "# Create a complete count including all categories (even those with 0 counts)\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Count actual assignments\n",
    "actual_counts = Counter(primary_aspects)\n",
    "\n",
    "# Create complete counts dictionary with all categories initialized to 0\n",
    "complete_counts = {category: 0 for category in all_categories}\n",
    "complete_counts['unclassified'] = 0  # Add unclassified as well\n",
    "\n",
    "# Update with actual counts\n",
    "for category, count in actual_counts.items():\n",
    "    complete_counts[category] = count\n",
    "\n",
    "print(\"\\n=== COMPLETE PRIMARY ASPECT COUNTS (ALL CATEGORIES) ===\")\n",
    "# Sort by count (descending) then by name for ties\n",
    "sorted_counts = sorted(complete_counts.items(), key=lambda x: (-x[1], x[0]))\n",
    "for category, count in sorted_counts:\n",
    "    print(f\"{category}: {count}\")\n",
    "\n",
    "print(f\"\\nTotal comments processed: {len(primary_aspects)}\")\n",
    "\n",
    "# Show percentage distribution (only for categories with counts > 0)\n",
    "print(\"\\n=== PERCENTAGE DISTRIBUTION (NON-ZERO CATEGORIES) ===\")\n",
    "total_comments = len(primary_aspects)\n",
    "for category, count in sorted_counts:\n",
    "    if count > 0:\n",
    "        percentage = (count / total_comments) * 100\n",
    "        print(f\"{category}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Summary of category usage\n",
    "used_categories = sum(1 for count in complete_counts.values() if count > 0)\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "print(f\"Categories with comments: {used_categories}\")\n",
    "print(f\"Categories with zero comments: {len(complete_counts) - used_categories}\")\n",
    "print(f\"Total categories defined: {len(complete_counts)}\")\n",
    "\n",
    "# ===== RAW SCORE ANALYSIS =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== RAW SCORE DISTRIBUTION ANALYSIS ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract all raw scores into a DataFrame for analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create DataFrame with all scores for each comment\n",
    "score_data = []\n",
    "for i, result in enumerate(aspect_results):\n",
    "    row = {'comment_id': i}\n",
    "    row.update(result['scores'])\n",
    "    score_data.append(row)\n",
    "\n",
    "scores_df = pd.DataFrame(score_data)\n",
    "\n",
    "# Remove comment_id column for calculations\n",
    "score_columns = [col for col in scores_df.columns if col != 'comment_id']\n",
    "\n",
    "print(f\"\\n=== TOTAL SCORES PER CATEGORY (Sum across all comments) ===\")\n",
    "total_scores = scores_df[score_columns].sum().sort_values(ascending=False)\n",
    "for category, total_score in total_scores.items():\n",
    "    print(f\"{category}: {total_score}\")\n",
    "\n",
    "print(f\"\\n=== AVERAGE SCORES PER CATEGORY ===\")\n",
    "avg_scores = scores_df[score_columns].mean().sort_values(ascending=False)\n",
    "for category, avg_score in avg_scores.items():\n",
    "    print(f\"{category}: {avg_score:.2f}\")\n",
    "\n",
    "print(f\"\\n=== COMMENTS WITH NON-ZERO SCORES PER CATEGORY ===\")\n",
    "nonzero_counts = (scores_df[score_columns] > 0).sum().sort_values(ascending=False)\n",
    "for category, count in nonzero_counts.items():\n",
    "    percentage = (count / len(scores_df)) * 100\n",
    "    print(f\"{category}: {count} comments ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n=== SCORE DISTRIBUTION STATISTICS ===\")\n",
    "print(\"Category | Min | Max | Mean | Std | 75th | 90th | 95th\")\n",
    "print(\"-\" * 60)\n",
    "for category in score_columns:\n",
    "    scores = scores_df[category]\n",
    "    stats = {\n",
    "        'min': scores.min(),\n",
    "        'max': scores.max(), \n",
    "        'mean': scores.mean(),\n",
    "        'std': scores.std(),\n",
    "        'p75': scores.quantile(0.75),\n",
    "        'p90': scores.quantile(0.90),\n",
    "        'p95': scores.quantile(0.95)\n",
    "    }\n",
    "    print(f\"{category[:15]:<15} | {stats['min']:3.0f} | {stats['max']:3.0f} | {stats['mean']:4.1f} | {stats['std']:4.1f} | {stats['p75']:4.1f} | {stats['p90']:4.1f} | {stats['p95']:4.1f}\")\n",
    "\n",
    "print(f\"\\n=== HIGH-SCORING COMMENTS (Score >= 5) ===\")\n",
    "for category in score_columns:\n",
    "    high_score_comments = scores_df[scores_df[category] >= 5]\n",
    "    if len(high_score_comments) > 0:\n",
    "        print(f\"\\n{category}: {len(high_score_comments)} comments with score >= 5\")\n",
    "        print(f\"  Max score: {scores_df[category].max()}\")\n",
    "        print(f\"  Score distribution: {scores_df[category][scores_df[category] >= 5].value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "print(f\"\\n=== MULTI-CATEGORY OVERLAP ANALYSIS ===\")\n",
    "# Find comments that score in multiple categories\n",
    "scores_binary = (scores_df[score_columns] > 0).astype(int)\n",
    "category_counts_per_comment = scores_binary.sum(axis=1)\n",
    "\n",
    "print(f\"Comments hitting multiple categories:\")\n",
    "overlap_counts = category_counts_per_comment.value_counts().sort_index()\n",
    "for num_categories, num_comments in overlap_counts.items():\n",
    "    percentage = (num_comments / len(scores_df)) * 100\n",
    "    print(f\"  {num_categories} categories: {num_comments} comments ({percentage:.1f}%)\")\n",
    "\n",
    "# Show most common category combinations for comments hitting multiple categories\n",
    "print(f\"\\n=== MOST COMMON CATEGORY COMBINATIONS (for multi-category comments) ===\")\n",
    "multi_category_comments = scores_df[category_counts_per_comment > 1]\n",
    "if len(multi_category_comments) > 0:\n",
    "    # Create combination strings for comments with multiple categories\n",
    "    combinations = []\n",
    "    for idx, row in multi_category_comments.iterrows():\n",
    "        active_categories = [cat for cat in score_columns if row[cat] > 0]\n",
    "        if len(active_categories) > 1:\n",
    "            combinations.append(\" + \".join(sorted(active_categories)))\n",
    "    \n",
    "    if combinations:\n",
    "        from collections import Counter\n",
    "        combo_counts = Counter(combinations)\n",
    "        print(\"Top 10 category combinations:\")\n",
    "        for combo, count in combo_counts.most_common(10):\n",
    "            percentage = (count / len(combinations)) * 100\n",
    "            print(f\"  {combo}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n=== THRESHOLD SENSITIVITY ANALYSIS ===\")\n",
    "print(\"Threshold | Unclassified | Classified | % Classified\")\n",
    "print(\"-\" * 50)\n",
    "for threshold in [1, 2, 3, 4, 5]:\n",
    "    unclassified_count = 0\n",
    "    for result in aspect_results:\n",
    "        max_score = max(result['scores'].values())\n",
    "        if max_score < threshold:\n",
    "            unclassified_count += 1\n",
    "    \n",
    "    classified_count = len(aspect_results) - unclassified_count\n",
    "    classified_pct = (classified_count / len(aspect_results)) * 100\n",
    "    print(f\"    {threshold}     |     {unclassified_count}      |     {classified_count}     | {classified_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69779f68",
   "metadata": {},
   "source": [
    "### Approach 3: Combining the above 2 approaches in a Hybrid Approach\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "7351e78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 10:48:59,317 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting on 349 comments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e9c651e3804f8c9eaa0de1c0ead613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 10:49:01,477 - BERTopic - Embedding - Completed ✓\n",
      "2025-08-06 10:49:01,478 - BERTopic - Guided - Find embeddings highly related to seeded topics.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e284dcf7f49549a49de68fd9333f215a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 10:49:01,492 - BERTopic - Guided - Completed ✓\n",
      "2025-08-06 10:49:01,492 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-08-06 10:49:01,637 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-08-06 10:49:01,638 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-08-06 10:49:01,651 - BERTopic - Cluster - Completed ✓\n",
      "2025-08-06 10:49:01,653 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-08-06 10:49:01,665 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top 10 topic overview ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>CustomName</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>111</td>\n",
       "      <td>-1_company_helpful_media_service</td>\n",
       "      <td>Agent Personality</td>\n",
       "      <td>[company, helpful, media, service, good, packa...</td>\n",
       "      <td>[company Media guy very helpful., I phone comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0_friendly_helpful_explained_polite</td>\n",
       "      <td>Communication Support</td>\n",
       "      <td>[friendly, helpful, explained, polite, profess...</td>\n",
       "      <td>[The advisor explained everything to me clearl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>1_service_customer_good_great</td>\n",
       "      <td>Support Actions</td>\n",
       "      <td>[service, customer, good, great, quick, effici...</td>\n",
       "      <td>[Good customer service, Your customer service ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>2_broadband_told_company_service</td>\n",
       "      <td>Process Experience</td>\n",
       "      <td>[broadband, told, company, service, customer, ...</td>\n",
       "      <td>[The router did not cover the whole of the hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>3_installation_engineer_came_helpful</td>\n",
       "      <td>Service Content</td>\n",
       "      <td>[installation, engineer, came, helpful, instal...</td>\n",
       "      <td>[Mat was incredibly helpful and friendly made ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                  Name             CustomName                                     Representation                                Representative_Docs\n",
       "0     -1    111      -1_company_helpful_media_service      Agent Personality  [company, helpful, media, service, good, packa...  [company Media guy very helpful., I phone comp...\n",
       "1      0     73   0_friendly_helpful_explained_polite  Communication Support  [friendly, helpful, explained, polite, profess...  [The advisor explained everything to me clearl...\n",
       "2      1     69         1_service_customer_good_great        Support Actions  [service, customer, good, great, quick, effici...  [Good customer service, Your customer service ...\n",
       "3      2     64      2_broadband_told_company_service     Process Experience  [broadband, told, company, service, customer, ...  [The router did not cover the whole of the hou...\n",
       "4      3     32  3_installation_engineer_came_helpful        Service Content  [installation, engineer, came, helpful, instal...  [Mat was incredibly helpful and friendly made ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Prepare your documents\n",
    "docs_df = df_clean.dropna(subset=[\"LTR_COMMENT\"])\n",
    "docs    = docs_df[\"LTR_COMMENT\"].tolist()\n",
    "print(f\"Fitting on {len(docs)} comments\")\n",
    "\n",
    "# 2) Your 7 seed‐topic keyword lists (as before)\n",
    "category_seed_keywords = {\n",
    "    \"Agent Personality\"     : [\"professional\",\"friendly\",\"helpful\",\"rude\",\"unhelpful\"],\n",
    "    \"Communication Support\" : [\"explained\",\"clear\",\"understood\",\"confusing\",\"outlined\"],\n",
    "    \"Support Actions\"       : [\"help\",\"support\",\"resolved\",\"assistance\"],\n",
    "    \"Process Experience\"    : [\"quick\",\"efficient\",\"waiting\",\"slow\",\"easy\",\"difficult\"],\n",
    "    \"Service Content\"       : [\"broadband\",\"internet\",\"wifi\",\"price\",\"bill\",\"package\"],\n",
    "    \"Competitive Retention\" : [\"sky\",\"bt\",\"virgin\",\"switching\",\"cancel\"],\n",
    "    \"Contact Patterns\"      : [\"callback\",\"follow\",\"again\",\"second\"]\n",
    "}\n",
    "seed_topic_list = list(category_seed_keywords.values())\n",
    "\n",
    "# 3) Instantiate vectorizer, UMAP, **and** HDBSCAN _with_ prediction_data\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "umap_model       = UMAP(n_neighbors=15, min_dist=0.0, metric=\"cosine\")\n",
    "hdbscan_model    = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=10,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True               # ← must have this for calculate_probabilities=True :contentReference[oaicite:0]{index=0}\n",
    ")\n",
    "\n",
    "# 4) Configure & fit BERTopic\n",
    "topic_model = BERTopic(\n",
    "    seed_topic_list       = seed_topic_list,\n",
    "    vectorizer_model      = vectorizer_model,\n",
    "    umap_model            = umap_model,\n",
    "    hdbscan_model         = hdbscan_model,\n",
    "    calculate_probabilities = True,    # now works without error\n",
    "    verbose               = True\n",
    ")\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# 5) Rename topics 0–6 to your human labels\n",
    "auto_labels = topic_model.generate_topic_labels(nr_words=3)\n",
    "custom_labels = [\n",
    "    list(category_seed_keywords.keys())[i] if i < len(seed_topic_list) else lbl\n",
    "    for i, lbl in enumerate(auto_labels)\n",
    "]\n",
    "topic_model.set_topic_labels(custom_labels)\n",
    "\n",
    "# 6) Quick check\n",
    "print(\"\\n=== Top 10 topic overview ===\")\n",
    "display(topic_model.get_topic_info().head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d6af3",
   "metadata": {},
   "source": [
    "Let's start finetuning. We'll adjust the threshold, number of neighbours and minimum cluster size. We'll also change our category seed words to be a bit more nuanced than before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "b080c745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Topic distribution (multi-label) ===\n",
      "1_service_customer_good_great : 162\n",
      "2_broadband_told_company_service : 149\n",
      "0_friendly_helpful_explained_polite : 115\n",
      "3_installation_engineer_came_helpful :  59\n",
      "\n",
      "=== Random sample of 10 comments with assigned topics ===\n",
      "\n",
      "1. COMMENT (truncated): Agent was very helpful from start to finish and made sure we got the right package for us at the best price....\n",
      "   → Topics: ['0_friendly_helpful_explained_polite', '2_broadband_told_company_service', '3_installation_engineer_came_helpful']\n",
      "\n",
      "2. COMMENT (truncated): Quick and efficient and good coverage...\n",
      "   → Topics: ['1_service_customer_good_great']\n",
      "\n",
      "3. COMMENT (truncated): The person on the phone was polite and gave me all the information I needed to make the right choice of which broadband was the best one for me...\n",
      "   → Topics: ['2_broadband_told_company_service']\n",
      "\n",
      "4. COMMENT (truncated): Very polite & extremely helpful , great service...\n",
      "   → Topics: ['0_friendly_helpful_explained_polite', '1_service_customer_good_great']\n",
      "\n",
      "5. COMMENT (truncated): Good service much cheaper than sky...\n",
      "   → Topics: ['2_broadband_told_company_service']\n",
      "\n",
      "6. COMMENT (truncated): Communications terrible...\n",
      "   → Topics: ['2_broadband_told_company_service']\n",
      "\n",
      "7. COMMENT (truncated): Not happy with way contract was sold to me...\n",
      "   → Topics: ['2_broadband_told_company_service']\n",
      "\n",
      "8. COMMENT (truncated): Well, I give a 4 because the set up went fairly smooth but the next day I call in the c.s to update my bank details and found out that there is an ongoing broadband issue in my area....\n",
      "   → Topics: ['2_broadband_told_company_service']\n",
      "\n",
      "9. COMMENT (truncated): Despite having entered all my data online I then had to go through everything on the phone It was extremely difficult to understand the advisor...\n",
      "   → Topics: ['2_broadband_told_company_service']\n",
      "\n",
      "10. COMMENT (truncated): Representative very helpful, gave good advice, understood my needs...\n",
      "   → Topics: ['0_friendly_helpful_explained_polite', '1_service_customer_good_great']\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# STEP 2 ― Multi-topic assignment + quick sanity checks\n",
    "# -----------------------------------------------------------\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Choose a soft membership threshold; 0.15 usually keeps 2-3 topics\n",
    "THRESHOLD = 0.15\n",
    "\n",
    "# Map each comment → list of topic labels with prob ≥ threshold\n",
    "multi_topic_map = defaultdict(list)\n",
    "for idx, (topic_probs) in enumerate(probs):\n",
    "    for t_id, p in enumerate(topic_probs):\n",
    "        if p >= THRESHOLD:\n",
    "            multi_topic_map[idx].append(topic_model.topic_labels_[t_id])\n",
    "\n",
    "# Add to the dataframe\n",
    "docs_df[\"topics_assigned\"] = [multi_topic_map[i] for i in range(len(docs_df))]\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1 ) Distribution of topic hits (one comment can count multiple times)\n",
    "# -----------------------------------------------------------\n",
    "flat_labels = [label for labels in docs_df[\"topics_assigned\"] for label in labels]\n",
    "label_counts = Counter(flat_labels)\n",
    "print(\"=== Topic distribution (multi-label) ===\")\n",
    "for label, cnt in label_counts.most_common():\n",
    "    print(f\"{label:<22} : {cnt:3d}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2 ) Random 10-comment sanity peek\n",
    "# -----------------------------------------------------------\n",
    "SAMPLE_N = 10\n",
    "sample_rows = docs_df.sample(SAMPLE_N, random_state=42)\n",
    "\n",
    "print(f\"\\n=== Random sample of {SAMPLE_N} comments with assigned topics ===\")\n",
    "for i, (_, row) in enumerate(sample_rows.iterrows(), 1):\n",
    "    print(f\"\\n{i}. COMMENT (truncated): {row['LTR_COMMENT'][:200]}...\")\n",
    "    print(f\"   → Topics: {row['topics_assigned']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c65efe1",
   "metadata": {},
   "source": [
    "#### Step 3: Hyperparameter tuning & Seed‐word Refinement\n",
    "\n",
    "Now that we’ve got an initial set of topics, we’ll iterate on:\n",
    "\n",
    "1. **Probability threshold** (`THRESHOLD`) for multi-label assignment  \n",
    "2. **UMAP neighbors** (`n_neighbors`) to control embedding granularity  \n",
    "3. **HDBSCAN minimum cluster size** (`min_cluster_size`) to control topic breadth  \n",
    "4. **Seed words**, making them more nuanced, integrating what we saw in the keyword analysis (Approach 2)\n",
    "\n",
    "Our goal is to see how these choices affect:\n",
    "- The number and focus of topics  \n",
    "- How many comments get 0, 1, or multiple topic tags  \n",
    "- Whether the random‐sample assignments “look right”\n",
    "\n",
    "Below, we define our new parameter grid, run a single tuning pass, and inspect the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "02ef12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3A) Define new hyperparameters & refined seed words\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# 1. Probability threshold for multi-labeling\n",
    "#THRESHOLD = 0.11    # lower → more topics per comment\n",
    "# we'll lower threshold from 0.10 to 0.08 since some aren't getting any topics\n",
    "\n",
    "\n",
    "# 2. UMAP & HDBSCAN settings\n",
    "# n_neighbors seems good around 10, 15 starts leaving out some topics when they are obviously present, but this is mitigated by lowering threshold\n",
    "# min_dist 0.0 seems to work well for keeping topics close together, but can try 0.1 if too many topics, We'll actually try 0.05 since this balances well\n",
    "# min_dist seems to be best knob for adjusting number of topics, settling on 0.12\n",
    "# min_cluster_size 5 seems to work well\n",
    "\n",
    "# 1) UMAP ― more global, looser grouping\n",
    "# 1) UMAP: more local detail\n",
    "umap_params = {\n",
    "    \"n_neighbors\": 12,\n",
    "    \"min_dist\"   : 0.05\n",
    "}\n",
    "\n",
    "# 2) HDBSCAN: allow moderate clusters\n",
    "hdbscan_params = {\n",
    "    \"min_cluster_size\": 6 # this is very sensitive\n",
    "}\n",
    "\n",
    "# 3) Multi-label threshold: pick up mid-strength signals\n",
    "THRESHOLD = 0.09\n",
    "\n",
    "# 4) Refined seed lists (split Service, remove generic overlaps)\n",
    "category_seed_keywords = {\n",
    "    \"Agent Personality\": [\n",
    "        \"friendly\", \"professional\", \"courteous\", \"polite\", \"patient\",\n",
    "        \"warm\", \"approachable\", \"dismissive\", \"cold\", \"arrogant\", \"rude\",\n",
    "        \"empathetic\", \"detached\"\n",
    "    ],\n",
    "    \"Communication Support\": [\n",
    "        \"explain\", \"clarify\", \"detailed\", \"transparent\", \"jargon\", \n",
    "        \"layperson\", \"straightforward\", \"opaque\", \"miscommunicated\", \n",
    "        \"misleading\", \"verbose\", \"concise\"\n",
    "    ],\n",
    "    \"Support Actions\": [\n",
    "        \"helpful\", \"resolve\", \"assist\", \"guide\", \"follow up\",\n",
    "        \"handled\", \"actioned\", \"escalated\", \"proactive\", \"reactive\",\n",
    "        \"triaged\", \"investigated\"\n",
    "    ],\n",
    "    \"Process Experience\": [\n",
    "        \"quick\", \"efficient\", \"smooth\", \"frictionless\", \"delay\", \n",
    "        \"hold\", \"queue\", \"timeout\", \"bottleneck\", \"hassle\", \"complicated\",\n",
    "        \"streamlined\", \"clunky\"\n",
    "    ],\n",
    "    \"Product Content\": [\n",
    "        \"broadband\", \"internet\", \"wifi\", \"speed\", \"latency\",\n",
    "        \"throughput\", \"upgrade\", \"downgrade\", \"signal\", \"connection\"\n",
    "    ],\n",
    "    \"Billing & Contracts\": [\n",
    "        \"billing\", \"price\", \"plan\", \"data cap\", \"charge\",\n",
    "        \"paperwork\", \"T&C\", \"auto-renew\", \"termination\", \"fine print\",\n",
    "        \"miscost\", \"prorated\", \"refund\", \"expensive\", \"value for money\", \"cost\"\n",
    "    ],\n",
    "    \"Competitive Retention\": [\n",
    "        \"sky\", \"virgin\", \"bt\", \"switch\", \"cancel\", \"loyalty\", \"match\",\n",
    "        \"offer\", \"churn\", \"winback\", \"exit\", \"incentive\", \"retention\",\n",
    "        \"counteroffer\"\n",
    "    ],\n",
    "    \"Contact Patterns\": [\n",
    "        \"callback\", \"ring\", \"call again\", \"voicemail\", \"dropped\",\n",
    "        \"redial\", \"repeat\", \"escalation\", \"timeout\", \"wait time\",\n",
    "        \"ringback\", \"pickup\"\n",
    "    ],\n",
    "    \"Contract Experience\": [\n",
    "        \"sold\", \"terms\", \"fine print\", \"mis-sold\", \"upsell\",\n",
    "        \"transfer\", \"port\", \"agreement\", \"renewal\"\n",
    "    ]\n",
    "}\n",
    "# I think the algo actually works better without the key words, so we'll try without them.\n",
    "\n",
    "\n",
    "# 4) Multi‐label threshold ― capture more nuance\n",
    "#THRESHOLD = 0.09   # lower to tag more topics per comment\n",
    "# we can also try without manually defined seed topics, but this seems to work well.\n",
    "\n",
    "# we'll merge agent personality with support actions, \n",
    "seed_topic_list = list(category_seed_keywords.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36bc6e2",
   "metadata": {},
   "source": [
    "### 3B) Re-fit BERTopic with new settings\n",
    "\n",
    "Below we re-initialize our topic model with the updated UMAP/HDBSCAN params and seed words, then re-compute topics and multi-label assignments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "93b29a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 10:49:01,719 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4dae96d37c49959b8f4813e2728f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 10:49:03,942 - BERTopic - Embedding - Completed ✓\n",
      "2025-08-06 10:49:03,943 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-08-06 10:49:04,055 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-08-06 10:49:04,056 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-08-06 10:49:04,073 - BERTopic - Cluster - Completed ✓\n",
      "2025-08-06 10:49:04,075 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-08-06 10:49:04,091 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3B) Re‐fit BERTopic with proper UMAP & HDBSCAN instances\n",
    "# -----------------------------------------------------------\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# --- 1) Instantiate vectorizer, UMAP & HDBSCAN properly ---\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=umap_params[\"n_neighbors\"],\n",
    "    min_dist=umap_params[\"min_dist\"],\n",
    "    metric=\"cosine\"\n",
    ")\n",
    "\n",
    "hdbscan_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=hdbscan_params[\"min_cluster_size\"],\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "# --- 2) Initialize the tuned BERTopic model ---\n",
    "tuned_model = BERTopic(\n",
    "    #seed_topic_list         = seed_topic_list,\n",
    "    vectorizer_model        = vectorizer_model,\n",
    "    umap_model              = umap_model,       # now a UMAP instance\n",
    "    hdbscan_model           = hdbscan_model,    # now an HDBSCAN instance\n",
    "    calculate_probabilities = True,\n",
    "    verbose                 = True\n",
    ")\n",
    "\n",
    "# --- 3) Fit & transform on all comments ---\n",
    "docs   = docs_df[\"LTR_COMMENT\"].tolist()\n",
    "topics, probs = tuned_model.fit_transform(docs)\n",
    "\n",
    "# --- 4) Re‐apply our custom labels ---\n",
    "auto_labels = tuned_model.generate_topic_labels(nr_words=3)\n",
    "custom_labels = [\n",
    "    list(category_seed_keywords.keys())[i]\n",
    "    if i < len(category_seed_keywords) else auto_labels[i]\n",
    "    for i in range(len(auto_labels))\n",
    "]\n",
    "tuned_model.set_topic_labels(custom_labels)\n",
    "\n",
    "# --- 5) Multi‐label assignment with updated THRESHOLD ---\n",
    "multi_topic_map = defaultdict(list)\n",
    "for idx, topic_probs in enumerate(probs):\n",
    "    for t_id, p in enumerate(topic_probs):\n",
    "        if p >= THRESHOLD:\n",
    "            multi_topic_map[idx].append(tuned_model.topic_labels_[t_id])\n",
    "\n",
    "docs_df[\"topics_assigned\"] = [multi_topic_map[i] for i in range(len(docs_df))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "926f0f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tuned Topic Distribution (multi-label) ===\n",
      "0_helpful_friendly_staff_explained :  75\n",
      "13_needed_friendly_contract_competence :  63\n",
      "1_customer_service_good_excellent :  37\n",
      "11_agent_helpful_start_polite :  33\n",
      "2_contract_emails_centers_operators :  33\n",
      "4_days_home_told_day      :  29\n",
      "5_phone_50_internet_number :  26\n",
      "6_quick_efficient_service_coverage :  26\n",
      "7_engineer_technician_mat_incredibly :  23\n",
      "9_price_reasonable_good_prices :  22\n",
      "8_broadband_company_april_media :  21\n",
      "3_installation_contact_experience_setup :  21\n",
      "10_media_company_recommend_friends :  15\n",
      "12_sky_wifi_18_3x         :  12\n",
      "\n",
      "=== Random sample of 10 comments (tuned) ===\n",
      "\n",
      "1. Agent was very helpful from start to finish and made sure we got the right package for us at the best price.\n",
      "   → Topics: ['11_agent_helpful_start_polite']\n",
      "\n",
      "2. Quick and efficient and good coverage\n",
      "   → Topics: ['6_quick_efficient_service_coverage']\n",
      "\n",
      "3. The person on the phone was polite and gave me all the information I needed to make the right choice of which broadband was the best one for me\n",
      "   → Topics: ['8_broadband_company_april_media']\n",
      "\n",
      "4. Very polite & extremely helpful , great service\n",
      "   → Topics: ['0_helpful_friendly_staff_explained', '13_needed_friendly_contract_competence']\n",
      "\n",
      "5. Good service much cheaper than sky\n",
      "   → Topics: ['12_sky_wifi_18_3x']\n",
      "\n",
      "6. Communications terrible\n",
      "   → Topics: ['2_contract_emails_centers_operators']\n",
      "\n",
      "7. Not happy with way contract was sold to me\n",
      "   → Topics: ['2_contract_emails_centers_operators', '11_agent_helpful_start_polite']\n",
      "\n",
      "8. Well, I give a 4 because the set up went fairly smooth but the next day I call in the c.s to update my bank details and found out that there is an ongoing broadband issue in my area.\n",
      "   → Topics: ['8_broadband_company_april_media']\n",
      "\n",
      "9. Despite having entered all my data online I then had to go through everything on the phone It was extremely difficult to understand the advisor\n",
      "   → Topics: ['0_helpful_friendly_staff_explained', '13_needed_friendly_contract_competence']\n",
      "\n",
      "10. Representative very helpful, gave good advice, understood my needs\n",
      "   → Topics: ['0_helpful_friendly_staff_explained']\n"
     ]
    }
   ],
   "source": [
    "### 3C) Inspect tuned results\n",
    "\n",
    "## 1. **Distribution** of topics  \n",
    "## 2. **Random sample** of 10 comments with their new topic tags\n",
    "\n",
    "# Distribution\n",
    "flat_labels  = [lbl for labels in docs_df[\"topics_assigned\"] for lbl in labels]\n",
    "label_counts = Counter(flat_labels)\n",
    "\n",
    "print(\"=== Tuned Topic Distribution (multi-label) ===\")\n",
    "for label, cnt in label_counts.most_common():\n",
    "    print(f\"{label:<25} : {cnt:3d}\")\n",
    "\n",
    "# Random sample check\n",
    "print(\"\\n=== Random sample of 10 comments (tuned) ===\")\n",
    "for i, (_, row) in enumerate(docs_df.sample(10, random_state=42).iterrows(), 1):\n",
    "    print(f\"\\n{i}. {row['LTR_COMMENT']}\")\n",
    "    print(f\"   → Topics: {row['topics_assigned']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b0a292",
   "metadata": {},
   "source": [
    "This seems fairly good after a lot of parameter tuning. We opt to remove keyword seeding, since it was restricting the algorithm.\n",
    "\n",
    "Now let's try and label the extracted topics more meaningfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "dcea15db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Topic 0: 0_helpful_friendly_staff_explained (75 docs) ===\n",
      "Top words: helpful, friendly, staff, explained, patient, things, advisor, talked, nice, extremely\n",
      "1. The person who talked me through what was going to happen was very patient and talked me through all that wÃ¡s happening.\n",
      "2. It was explained so I could understand what was being said to me thank u to the lady I spoke to\n",
      "3. Professional service\n",
      "4. The girl I spoke to was brilliant, very helpful, understanding and got the job done.\n",
      "5. He was professional , came right in time .\n",
      "6. Great service. Friendly staff.\n",
      "7. Jake the representative for company was simply above and beyond helpful and done everything for me with ease\n",
      "8. Very friendly staff and accommodating\n",
      "9. Good communication and very helpful\n",
      "10. Very friendly and knowledgeable\n",
      "\n",
      "=== Topic 1: 1_customer_service_good_excellent (37 docs) ===\n",
      "Top words: customer, service, good, excellent, great, given, amazing, fantastic, hope, brilliant\n",
      "1. friendly and good customer service, plus a suitable product and easy to sign up.\n",
      "2. I have been a customer for over 20 years and have always had a very good service from you.\n",
      "3. Customer care sounded very nicely but the rule of reading too many things and expecting me to make decision based on that is really odd. Let me use the service before I think of recommendation Please\n",
      "4. Excellent service offed by the front desk\n",
      "5. Z, who guided me through the process on the phone, was the epitome of empathetic warm professionalism. If all of your customer service personnel are as good as her and your products remain competitive, I see myself becoming a long-term loyal customer.\n",
      "6. Amazing customer service by sade\n",
      "7. The customer service is on point and the offer I was given is excellent.\n",
      "8. Because I've always had a good relationship with company,until I came across unhelpful customer service employee the rest is history.\n",
      "9. Very good customer service\n",
      "10. Initial sale was great. Employee was knowledgeable and explained the process and benefits. I had trouble setting up the device when I received it, and the support staff were extremely helpful and got me up and running. The perfect experience!!!\n",
      "\n",
      "=== Topic 2: 2_contract_emails_centers_operators (33 docs) ===\n",
      "Top words: contract, emails, centers, operators, ive, tried, cancel, change, team, did\n",
      "1. Extremely disappointed with your customer service team. I am still awaiting a response from your fraud prevention team and I have waste hours of my time already on a service I have yet to receive.\n",
      "2. Far to many e-mails and messages, plus its only been two days since ordering the phone line which is not yet installed.\n",
      "3. i am a new customer and my contract was cancelled without notifying me, only because box did not arrive that i called to see what was happening. had to wait another 3 days for box to arrive.\n",
      "4. My internet isn¬\"t working your team are pushy and despite having put in account details to talk to someone on the phone you have to repeat them. It¬\"s a frustrating and soul destroying experience\n",
      "5. Your customer service is awful and you really need to ditch the overseas call centers. Kindness and understanding in the voice go a long way. You can feel the contempt in operator voices (sniggering and laughing when you say you cannot understnd what they are saying - feels intentional) and cannot quite get all of the words due to very strong accents. Your only saving grace is the physical connection and stability of that. Thankfully, because connection quality is good I rarely (if ever - Thank Goodness) have to contact your call centers. Would not touch with a barge pole otherwise. Take a look at Now Broadband whos operators are absolutely marvellous. Lessons learned or brushed under the carpet? Haha\n",
      "6. I joined newly and requested to change the package when I realised phone line is not included. The agent attended to me very well but those in charge of email kept sending me the earlier package. This must improve.\n",
      "7. When I made my choice to have company supplied to my new home I was lucky enough to have Niccy on the phone I¬\"m not a young person so sometimes one can get confused with technology, she explained everything in great detail she is a credit to your company and I thank her very much we have not received our kit as yet but I¬\"m sure it¬\"s on its way\n",
      "8. Not happy with way contract was sold to me\n",
      "9. You keep sending me messages for a completely different person, you tell me my equipment has been delivered, NO it hasn¬\"t, the emails have the totally wrong price on them so am NOT impressed at all, so where we go from here I don¬\"t know\n",
      "10. Customer service keep hanging up the call can¬\"t solve any of the problems overall sh*t service\n",
      "\n",
      "=== Topic 3: 3_installation_contact_experience_setup (21 docs) ===\n",
      "Top words: installation, contact, experience, setup, times, engineer, sure, make, installed, install\n",
      "1. Very helpful customer service - Rosie was a total star - and easy-to-install setup\n",
      "2. Installation is super fast and easy, delivery is also very fast. Internet speed is about just excellent despite the fact that it is between 500 and 800 instead of the 1company4 expected. The only concern is that the call to request the interned is long, it takes up to 1 hour.\n",
      "3. I am very unhappy to now read that there will be a £54 Installation Fee..... What is this? The apartment was already assigned to company by the previous Tennant. And no engineer came to install anything. I took delivery of the box and installed it myself.\n",
      "4. The engineer that came to help with the installation, Abbie, went above and beyond to understand how I wanted the setup and make sure everything was done properly. Very happy with his service.\n",
      "5. Excellent service from the get go, the gent I spoke to on the phone, and the guy who did the installation were superb. I can¬\"t remember their name though.\n",
      "6. Excellent advice in your contact centre, Very prompt contact from your installation agents (the day after our order was placed).\n",
      "7. Pleased with the Installation even after slight problems with dates for appointment which was sorted by the Manager.\n",
      "8. I have contacted regarding my login problem but it is not sorted yet.\n",
      "9. We have not been set up yet, but speaking to Ashley very informative and pleasant. On Saturday the chaps came a day did the road to house infrastructure, which was satisfactory. Today site management checked everything was installed satisfactory and we're we happy.\n",
      "10. installation man was very good\n",
      "\n",
      "=== Topic 4: 4_days_home_told_day (29 docs) ===\n",
      "Top words: days, home, told, day, company, work, internet, delivery, cancelling, house\n",
      "1. Accommodated my request for preferred cable routing. Work carried out swiftly.\n",
      "2. The cords to be connected to the wall unit is not of the same size\n",
      "3. Poor customer care, told to wait in for an engineer so did so all day and nobody arrived and no call to tell me this was going to happen.\n",
      "4. terrible customer service, a week later and my WIFI still isn't set up and nobody has contacted me since they said it would be sorted within 24 hrs and someone would call me\n",
      "5. I needed my SIM urgently and was told I will get it by Wednesday even though I called on Monday evening. I got my SIM on Wednesday as I was told I would.\n",
      "6. On the 13th March I ordered with Supanet, I contacted them again on the 3rd April to found out when I could expect my broadband, they had no answers, I immediately cancelled the order and contacted company sales, I spoke with a very helpful gentleman who informed me company would have me up and running in 2 days the 5th April, I immediately confirmed my order. Today is Wednesday 5th April I have received my parcel from company and my broadband is all connected, well done company, excellent service\n",
      "7. They were proactive and called me, as I had added to the Basket. Saved me a call. The next day I filled in the request for information outside our house for cabling. The contractor arrived a few hours later to fit the cable. Unbelievable service.\n",
      "8. Awful service and support. Work only sales. After - no any help. Site doesnt work. I am really disappointed\n",
      "9. company Told me I had cable so was told to cancel sky on Tuesday as company was getting set up , company man turnt up and there¬\"s no cable now have to wait more than 2 weeks with no internet, I have someone working from home that now can¬\"t work and my daughter that is home schooled that now can¬\"t do her lessons Thanks company I¬\"m not even with you yet and I¬\"m getting hassle¬¦!\n",
      "10. I had a massive drama with shell energy moving home. I has very bad customer service, I spoke to company and within 2 days had active broadband in my house. Something She¬\"ll couldn¬\"t do after 3 hrs on the phone\n",
      "\n",
      "=== Topic 5: 5_phone_50_internet_number (26 docs) ===\n",
      "Top words: phone, 50, internet, number, lady, dont, talk, online, took, helpful\n",
      "1. It took one & a half hours to sign up I'm 80 yrs old & found the experience exhausting !\n",
      "2. I have been a customer for over 20 years and have always had a very good service from you.\n",
      "3. When I finally was able to speak to someone. They were very helpful. Going forward I would suggest it would be helpful to have a phone number, more easily accessible. I came very close to giving up with the online signing.\n",
      "4. Very helpful on the phone, organising everything. Good service, the only thing missed was the PAC code for transferring my number to the new sim\n",
      "5. You wouldn¬\"t not let be add my referral link after set up\n",
      "6. Helpful, easy to talk to, not pushy to get me to commit, good deals was willing to call me back when I was free. Patent when I was deciding on which deal I wanted explained everything I needed to know.\n",
      "7. Superb service with a very helpful agent.\n",
      "8. Polite and helpful phone staff that reached out and made an extra effort to call me back and help me move my Internet address when I hadn't even requested. They knew to reach out because of me searching the post code of the next address via company media website. They went out of their way to help me change account details as well and was very kind.\n",
      "9. Because Chris was amazing when she contacted me after I had put my details online.\n",
      "10. Far to many e-mails and messages, plus its only been two days since ordering the phone line which is not yet installed.\n",
      "\n",
      "=== Topic 6: 6_quick_efficient_service_coverage (26 docs) ===\n",
      "Top words: quick, efficient, service, coverage, friendly, easy, good, detailed, complicated, responsive\n",
      "1. Good affordable service Staff helpful\n",
      "2. Very efficient, friendly but proffessional experience from initial phone call to fitting. Thankyou\n",
      "3. Quick, efficient service with easy set up.\n",
      "4. Good friendly service quick and easy\n",
      "5. Great service, Quick set up!\n",
      "6. Amazing service and job completed\n",
      "7. Excellent Support .. made it so easy\n",
      "8. Fast service and coverage\n",
      "9. Quick and efficient and good coverage\n",
      "10. Very efficient and Friendly Service\n",
      "\n",
      "=== Topic 7: 7_engineer_technician_mat_incredibly (23 docs) ===\n",
      "Top words: engineer, technician, mat, incredibly, cool, professional, process, great, friendly, helpful\n",
      "1. Very friendly engineer who answered our questions kindly and installed our system without hassle and on time.\n",
      "2. The engineer that attended my house was very friendly with great sense of humour\n",
      "3. Great service - engineer came out the next day . He was lovely and got me up And running in no time\n",
      "4. Mat was brilliant, followed up with me when I was advailabe. Really professional and plesent to deal with.\n",
      "5. The technician was excellent\n",
      "6. Very professional, very kind and great customer service from the engineer\n",
      "7. Confirmation procedure is strange Robot detection is pathetic Minute pictures American dialect! I am 77 and failing consistently to pass this fantastic test\n",
      "8. The technicians and phone support team have been very nice, however the connection just isn't what we expected. The product is quite average.\n",
      "9. Extremely helpful engineer on visit Jo 32725\n",
      "10. The engineer Steve was on time friendly and helpful ðx_x0018_\n",
      "\n",
      "=== Topic 8: 8_broadband_company_april_media (21 docs) ===\n",
      "Top words: broadband, company, april, media, immediately, true, 5th, order, contacted, connected\n",
      "1. All good, but I never give 10 for anything.\n",
      "2. Very helpful and positive response when I rang to set up broadband - thank you.\n",
      "3. On the 13th March I ordered with Supanet, I contacted them again on the 3rd April to found out when I could expect my broadband, they had no answers, I immediately cancelled the order and contacted company sales, I spoke with a very helpful gentleman who informed me company would have me up and running in 2 days the 5th April, I immediately confirmed my order. Today is Wednesday 5th April I have received my parcel from company and my broadband is all connected, well done company, excellent service\n",
      "4. Well, I give a 4 because the set up went fairly smooth but the next day I call in the c.s to update my bank details and found out that there is an ongoing broadband issue in my area.\n",
      "5. Agent was very helpful in setting me up with the new broadband a true gent\n",
      "6. Very help full got a great deal can't wait to install the broadband\n",
      "7. Haven't gave it a 10 because when I was setting up the TV and Broadband package with one of your employer's on your team they was going through the payments and bundle I was having and tried to charge me a set up fee when I new it was free could say it was just a mistake but if I didn't have my wit's about me and pointed out it was free of charge he would of charged me on my bill I let him check it out for himself but was put right and we moved on and carried out the deal I had Infront of me apart from that he was helpful .\n",
      "8. Good internet connection\n",
      "9. Ordered Thursday 20th April and was sent a confirmation for delivery Monday 24th. Delivery was late so I called up on Friday 28th, and was informed that my order had not gone through. Had to order again over the phone, with another credit check, and soonest delivery Tuesday 2nd May.\n",
      "10. Very helpful and efficient service and competitive broadband TV package\n",
      "\n",
      "=== Topic 9: 9_price_reasonable_good_prices (22 docs) ===\n",
      "Top words: price, reasonable, good, prices, great, value, alright, desk, pricey, reasonably\n",
      "1. Fantastic customer experience\n",
      "2. Great price - very competitive\n",
      "3. A good range of products at affordable prices\n",
      "4. Excellent on line service\n",
      "5. Good internet at the moment\n",
      "6. Brilliant customer service\n",
      "7. Great service and sales consultant..great deal offered\n",
      "8. Expensive but the best value around for the address\n",
      "9. Prompt response Swift delivery Good network Affordability\n",
      "10. It is alright reasonably good\n",
      "\n",
      "=== Topic 10: 10_media_company_recommend_friends (15 docs) ===\n",
      "Top words: media, company, recommend, friends, came, say, really, guy, nice, install\n",
      "1. The engineer that came to help with the installation, Abbie, went above and beyond to understand how I wanted the setup and make sure everything was done properly. Very happy with his service.\n",
      "2. installation man was very good\n",
      "3. The experience is really good. The price can be a bit expensive. But I have had good experience with company media so far. The engineer, who came to my address was very professional and friendly. And did a really good job running the wires nice and tidy. Exactly how I wanted.\n",
      "4. Very helpful customer service - Rosie was a total star - and easy-to-install setup\n",
      "5. company Media guy very helpful.\n",
      "6. Asking me about the service company Media supply, BEFORE I am connected is somewhat pointless. The Customer Support staff were very helpful , but the online mechanisms gave me a headache and I would certainly NOT recommend that mechanism to get a company Media service. Time will tells as to whether my score will increase.\n",
      "7. I am new to company media so I can't say yet but I liked the service when setting up my account.\n",
      "8. Great customer service, best wifi on the market, and I also like that I have heard that company engineers, use their own white box, wires, and install to completion.\n",
      "9. The service is superp I really appreciate the turnaround as well as the constant updates regarding progress. company Media team is so friendly and willing to assist. I will recommend company Media to my friends.\n",
      "10. The guy that came to install company media at my home was very pleasant and helpful I would recommend company media to my friends and family\n",
      "\n",
      "=== Topic 11: 11_agent_helpful_start_polite (33 docs) ===\n",
      "Top words: agent, helpful, start, polite, clear, took, went, process, dee, stated\n",
      "1. Agent was very helpful from start to finish and made sure we got the right package for us at the best price.\n",
      "2. Lady rang me when promised,helpful & friendly\n",
      "3. Angie was extremely helpful and helped me choose the right package for my needs (we think)\n",
      "4. The agent was very polite and helpful, but I am wondering why he hasn't sent me the contract papers with the details as he promised. I also like to remind him of one month free plus six months £15.00\n",
      "5. Because you keep sending me emails even after I delete them because I don¬\"t want to answer\n",
      "6. Olivia very help and friendly\n",
      "7. Relatively straightforward\n",
      "8. Your agent was helpful and polite. His clear explanation of the process,a great help to me . was\n",
      "9. Very patient call handler, explained everything in detail with good professionalism and polite voice tone throughout. Did not upsell and pressurised in get other services.\n",
      "10. The phone conversation I had with your representative was well explicit. He had already sorted my order out on phone. Great job.\n",
      "\n",
      "=== Topic 12: 12_sky_wifi_18_3x (12 docs) ===\n",
      "Top words: sky, wifi, 18, 3x, hd, guess, higher, helpfull, goodengineer, disappointedwe\n",
      "1. Good price it¬\"s the only reason I guess after the 18 months when you want to charge me more is when I shall go back to sky It¬\"s mad but has to be done\n",
      "2. Very good customer service from Agent and she found us a really good deal, a lot better than we was getting from sky\n",
      "3. Just felt treated better than with sky\n",
      "4. I've only just ordered my package from company and it won't be up and running until the 22nd April. I need to see how good it is before I can give top marks.\n",
      "5. Poor customer service!!!!!!! I¬\"ve been told twice I¬\"d get a call back but nothing has happened. Your website says all sky channels in HD?? Sky sports is not¬¦very misleading for customers.\n",
      "6. I was impressed with company but decided to stay with sky\n",
      "7. Price better than sky Better broadband speed Easy transition Agent was a great sales assistant . Instalation date quick. Happy with service. Looking forward to my company package.\n",
      "8. I'm extremely happy with how quickly you've got me up and running no messing about like Vodafone. I'd happily recommend you to anyone for a fast trouble free experience from start to finish thank you to all your team\n",
      "9. I got a good offer with company than sky\n",
      "10. Very satisfied upto now won't give a 10 until I know that it is up and running ok\n",
      "\n",
      "=== Topic 13: 13_needed_friendly_contract_competence (63 docs) ===\n",
      "Top words: needed, friendly, contract, competence, courtesy, surrownding, taker, promptness, crowded, helpfulness\n",
      "1. Polite, knowledgeable call staff. Explained everything really .\n",
      "2. Professional service\n",
      "3. My daughter went through everything with your operator as I am 8o years old and don't understand a lot of the language used technically. All I want is to be able to do what I know and enjoy it. My daughter is with you and praised your service highly.\n",
      "4. Easy set up process , very helpful staff on the phone.\n",
      "5. Good affordable service Staff helpful\n",
      "6. Superb service with a very helpful agent.\n",
      "7. Very polite & extremely helpful , great service\n",
      "8. The team who came out were friendly and explained all they needed to do. Also clear about the fact one of them was a trainee but still provided an excellent customer service\n",
      "9. Very polite and professional people who sorted my package out for me\n",
      "10. Clara was very helpful and extremely patient with all of my questions\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3D) Print up to 10 sample comments per topic for manual labeling\n",
    "# -----------------------------------------------------------\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Recompute distribution if needed\n",
    "flat_labels  = [lbl for labels in docs_df[\"topics_assigned\"] for lbl in labels]\n",
    "label_counts = Counter(flat_labels)\n",
    "\n",
    "NUM_SAMPLES = 10\n",
    "topic_info  = tuned_model.get_topic_info()\n",
    "topic_ids   = topic_info[topic_info.Topic != -1][\"Topic\"].tolist()\n",
    "\n",
    "for t_id in topic_ids:\n",
    "    label = tuned_model.topic_labels_[t_id]\n",
    "    count = label_counts[label]\n",
    "    # Get indices of docs assigned this topic\n",
    "    assigned_idxs = [i for i, labs in enumerate(docs_df[\"topics_assigned\"]) if label in labs]\n",
    "    if not assigned_idxs:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== Topic {t_id}: {label} ({count} docs) ===\")\n",
    "    # Show top words for context\n",
    "    top_words = [w for w, _ in tuned_model.get_topic(t_id)[:10]]\n",
    "    print(\"Top words:\", \", \".join(top_words))\n",
    "\n",
    "    # Sample up to NUM_SAMPLES comments\n",
    "    sampled = random.sample(assigned_idxs, min(NUM_SAMPLES, len(assigned_idxs)))\n",
    "    for i, doc_idx in enumerate(sampled, 1):\n",
    "        comment = docs_df.iloc[doc_idx][\"LTR_COMMENT\"].replace(\"\\n\", \" \")\n",
    "        snippet = comment#[:250] #+ (\"...\" if len(comment) > 250 else \"\")\n",
    "        print(f\"{i}. {snippet}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b972338",
   "metadata": {},
   "source": [
    "From the above, we can manually label what the topics are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "8459994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = {\n",
    "    \"0_helpful_friendly_staff_explained\": \"Staff Helpfulness and Communication\",\n",
    "    \"1_customer_service_good_excellent\": \"Overall Customer Service Quality\", \n",
    "    \"2_contract_emails_centers_operators\": \"Contract Issues and Call Center Problems\",\n",
    "    \"3_installation_contact_experience_setup\": \"Installation Process and Setup Experience\",\n",
    "    \"4_days_home_told_day\": \"Service Delivery and Scheduling Issues\",\n",
    "    \"5_phone_50_internet_number\": \"Phone Support and Account Management\",\n",
    "    \"6_quick_efficient_service_coverage\": \"Service Speed and Network Coverage\",\n",
    "    \"7_engineer_technician_mat_incredibly\": \"Field Engineer Performance\",\n",
    "    \"8_broadband_company_april_media\": \"Broadband Service and Connectivity\",\n",
    "    \"9_price_reasonable_good_prices\": \"Pricing and Value Proposition\",\n",
    "    \"10_media_company_recommend_friends\": \"Installation Quality and Recommendations\",\n",
    "    \"11_agent_helpful_start_polite\": \"Sales Agent Performance and Process\",\n",
    "    \"12_sky_wifi_18_3x\": \"Competitor Comparisons (Sky)\",\n",
    "    \"13_needed_friendly_contract_competence\": \"Professional Service Standards\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab570f5",
   "metadata": {},
   "source": [
    "##### Step 3E: Map to Substantive Labels & Add Topic Counts\n",
    "\n",
    "Now that we have our raw topic labels → substantive labels mapping, we’ll:\n",
    "\n",
    "1. **Map** each row’s raw `topics_assigned` list to your human-readable labels  \n",
    "2. **Count** how many topics each comment has  \n",
    "3. **Merge** these back into the original filtered dataframe, leaving untouched any rows that weren’t part of our pipeline  \n",
    "4. **Sanity-check** that the join happened correctly\n",
    "\n",
    "Below is a single code cell to do all of the above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "082a1091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments in original df_clean: 349\n",
      "Comments modeled (docs_df): 349\n",
      "Comments with >0 topics: 328\n",
      "\n",
      "=== Distribution of Number of Topics per Comment ===\n",
      "num_topics_assigned\n",
      "0     21\n",
      "1    242\n",
      "2     64\n",
      "3     22\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Sample rows with substantive topics and counts ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LTR_COMMENT</th>\n",
       "      <th>topics_assigned</th>\n",
       "      <th>substantive_topics</th>\n",
       "      <th>num_topics_assigned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>Very pleasant call centre agent. Very fast ins...</td>\n",
       "      <td>[13_needed_friendly_contract_competence]</td>\n",
       "      <td>[Professional Service Standards]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>Polite and helpful phone staff that reached ou...</td>\n",
       "      <td>[5_phone_50_internet_number, 11_agent_helpful_...</td>\n",
       "      <td>[Phone Support and Account Management, Sales A...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>The engineer Steve was on time friendly and he...</td>\n",
       "      <td>[7_engineer_technician_mat_incredibly]</td>\n",
       "      <td>[Field Engineer Performance]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>The gentleman jack kindly got me a good priced...</td>\n",
       "      <td>[13_needed_friendly_contract_competence]</td>\n",
       "      <td>[Professional Service Standards]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>We have not been set up yet, but speaking to A...</td>\n",
       "      <td>[3_installation_contact_experience_setup, 4_da...</td>\n",
       "      <td>[Installation Process and Setup Experience, Se...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           LTR_COMMENT                                    topics_assigned                                 substantive_topics  num_topics_assigned\n",
       "583  Very pleasant call centre agent. Very fast ins...           [13_needed_friendly_contract_competence]                   [Professional Service Standards]                    1\n",
       "314  Polite and helpful phone staff that reached ou...  [5_phone_50_internet_number, 11_agent_helpful_...  [Phone Support and Account Management, Sales A...                    2\n",
       "632  The engineer Steve was on time friendly and he...             [7_engineer_technician_mat_incredibly]                       [Field Engineer Performance]                    1\n",
       "286  The gentleman jack kindly got me a good priced...           [13_needed_friendly_contract_competence]                   [Professional Service Standards]                    1\n",
       "75   We have not been set up yet, but speaking to A...  [3_installation_contact_experience_setup, 4_da...  [Installation Process and Setup Experience, Se...                    2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3E) Apply mapping and add counts\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Start from the filtered 'docs_df' used for topic modeling\n",
    "final_df = df_clean.copy()\n",
    "\n",
    "# Sanity check: all docs_df indices should be in df_clean\n",
    "assert set(docs_df.index).issubset(final_df.index), \"Pipeline indices mismatch!\"\n",
    "\n",
    "# 1) Bring over raw topics_assigned\n",
    "final_df.loc[docs_df.index, \"topics_assigned\"] = docs_df[\"topics_assigned\"]\n",
    "\n",
    "# 2) Map to substantive labels\n",
    "final_df.loc[docs_df.index, \"substantive_topics\"] = docs_df[\"topics_assigned\"].map(\n",
    "    lambda lbls: [topic_labels.get(lbl, lbl) for lbl in lbls]\n",
    ")\n",
    "\n",
    "# 3) Add a count of how many topics each comment received\n",
    "final_df.loc[docs_df.index, \"num_topics_assigned\"] = docs_df[\"topics_assigned\"].map(len)\n",
    "\n",
    "# 4) Fill missing rows (that didn’t go through the pipeline) with defaults\n",
    "final_df[\"topics_assigned\"]      = final_df[\"topics_assigned\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "final_df[\"substantive_topics\"]   = final_df[\"substantive_topics\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "final_df[\"num_topics_assigned\"]  = final_df[\"num_topics_assigned\"].fillna(0).astype(int)\n",
    "\n",
    "# Sanity checks\n",
    "print(f\"Total comments in original df_clean: {len(df_clean)}\")\n",
    "print(f\"Comments modeled (docs_df): {len(docs_df)}\")\n",
    "print(f\"Comments with >0 topics: {sum(final_df['num_topics_assigned'] > 0)}\")\n",
    "\n",
    "# Quick look at distribution of topic‐counts\n",
    "print(\"\\n=== Distribution of Number of Topics per Comment ===\")\n",
    "print(final_df[\"num_topics_assigned\"].value_counts().sort_index())\n",
    "\n",
    "# Preview a few rows\n",
    "print(\"\\n=== Sample rows with substantive topics and counts ===\")\n",
    "display(final_df.loc[docs_df.index].sample(5)[\n",
    "    [\"LTR_COMMENT\", \"topics_assigned\", \"substantive_topics\", \"num_topics_assigned\"]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "b139f502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df shape: (582, 18)\n",
      "final_df shape      : (349, 25)\n",
      "Shared index count  : 349 \n",
      "\n",
      "Merged df shape          : (582, 21)\n",
      "Rows with ≥1 topic       : 328\n",
      "Rows with 0 topics       : 254\n",
      "Sum of topic counts      : 436 \n",
      "\n",
      "=== Sample merged rows ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LTR_COMMENT</th>\n",
       "      <th>topics_assigned</th>\n",
       "      <th>substantive_topics</th>\n",
       "      <th>num_topics_assigned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Price of my bundle went up 2 days after i plac...</td>\n",
       "      <td>[4_days_home_told_day, 8_broadband_company_apr...</td>\n",
       "      <td>[Service Delivery and Scheduling Issues, Broad...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>Good internet connection</td>\n",
       "      <td>[8_broadband_company_april_media]</td>\n",
       "      <td>[Broadband Service and Connectivity]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>I've not agreed to any of this yet I'm bombard...</td>\n",
       "      <td>[2_contract_emails_centers_operators]</td>\n",
       "      <td>[Contract Issues and Call Center Problems]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>Excellent advice in your contact centre, Very ...</td>\n",
       "      <td>[3_installation_contact_experience_setup]</td>\n",
       "      <td>[Installation Process and Setup Experience]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           LTR_COMMENT                                    topics_assigned                                 substantive_topics  num_topics_assigned\n",
       "51   Price of my bundle went up 2 days after i plac...  [4_days_home_told_day, 8_broadband_company_apr...  [Service Delivery and Scheduling Issues, Broad...                    2\n",
       "337                                                NaN                                                 []                                                 []                    0\n",
       "301                           Good internet connection                  [8_broadband_company_april_media]               [Broadband Service and Connectivity]                    1\n",
       "221  I've not agreed to any of this yet I'm bombard...              [2_contract_emails_centers_operators]         [Contract Issues and Call Center Problems]                    1\n",
       "262  Excellent advice in your contact centre, Very ...          [3_installation_contact_experience_setup]        [Installation Process and Setup Experience]                    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now we can merge final_df with the original df to get our original data with extracted topics\n",
    "\n",
    "df = pd.read_pickle(\"../data/processed/data_with_sentiment.pkl\") # we'll get our full data at this point\n",
    "\n",
    "# 1) Confirm index overlap\n",
    "print(\"Original df shape:\", df.shape)\n",
    "print(\"final_df shape      :\", final_df.shape)\n",
    "shared_idx = df.index.intersection(final_df.index)\n",
    "print(\"Shared index count  :\", len(shared_idx), \"\\n\")\n",
    "\n",
    "# 2) Perform the join\n",
    "merged_df = df.join(\n",
    "    final_df[[\"topics_assigned\", \"substantive_topics\", \"num_topics_assigned\"]],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 3) Fill defaults for rows without any topic assignment\n",
    "merged_df[\"topics_assigned\"] = merged_df[\"topics_assigned\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "merged_df[\"substantive_topics\"] = merged_df[\"substantive_topics\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "merged_df[\"num_topics_assigned\"] = merged_df[\"num_topics_assigned\"].fillna(0).astype(int)\n",
    "\n",
    "# 4) Sanity checks\n",
    "print(\"Merged df shape          :\", merged_df.shape)\n",
    "print(\"Rows with ≥1 topic       :\", (merged_df[\"num_topics_assigned\"] > 0).sum())\n",
    "print(\"Rows with 0 topics       :\", (merged_df[\"num_topics_assigned\"] == 0).sum())\n",
    "print(\"Sum of topic counts      :\", merged_df[\"num_topics_assigned\"].sum(), \"\\n\")\n",
    "\n",
    "# 5) Preview a few merged rows\n",
    "print(\"=== Sample merged rows ===\")\n",
    "display(merged_df.sample(5)[\n",
    "    [\"LTR_COMMENT\", \"topics_assigned\", \"substantive_topics\", \"num_topics_assigned\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b84d95c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to: ../data/processed/data_with_sentiment_and_topics.pkl\n"
     ]
    }
   ],
   "source": [
    "## Save our data with both sentiment and topics\n",
    "merged_df.to_pickle(\"../data/processed/data_with_sentiment_and_topics.pkl\")\n",
    "print(\"Merged data saved to: ../data/processed/data_with_sentiment_and_topics.pkl\")\n",
    "\n",
    "# save to csv\n",
    "merged_df.to_csv(\"../data/processed/data_with_sentiment_and_topics.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
