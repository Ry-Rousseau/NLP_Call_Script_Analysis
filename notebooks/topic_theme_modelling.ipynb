{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944df786",
   "metadata": {},
   "source": [
    "### Topic Modelling \n",
    "\n",
    "Overall Research Questions:\n",
    "1) How do customers define 'good' service, and how does the new script shift those definitions?\n",
    "2) What aspects of service (clarity, empathy, agent personality) drive variance in sentiment?\n",
    "3) Does the new script systematically change perceptions or emotional tone, particularly for high-value segments like VOLT?\n",
    "\n",
    "What are the best topic modelling approaches given the above?\n",
    "\n",
    "In addition, we'll focus on the following:\n",
    "\n",
    "- What are the top latent topics mentioned?\n",
    "- How do topic distributions differ by treatment?\n",
    "- What percentage of comments mention agent personality, clarity, or reassurance?\n",
    "\n",
    "Topic modelling is most relevant to Q1, but relates to Q3 as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251166c7",
   "metadata": {},
   "source": [
    "#### load the data/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d138d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main dataset loaded: (582, 15)\n",
      "VOLT customers: 241\n",
      "Non-VOLT customers: 341\n",
      "Treatment group: 247\n",
      "Control group: 335\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exec(open('../scripts/setup.py').read()) # load our data/packages\n",
    "# Text preprocessing functions are already defined in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "587549c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GROUP</th>\n",
       "      <th>VOLT_FLAG</th>\n",
       "      <th>SURVEY_ID</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>LTR_COMMENT</th>\n",
       "      <th>PRIMARY_REASON</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>CONNECTION_TIME</th>\n",
       "      <th>SALES_PERSON_SAT</th>\n",
       "      <th>SALES_FRIENDLY_SAT</th>\n",
       "      <th>COMMINICATION_SAT</th>\n",
       "      <th>FIRST_BILL_SAT</th>\n",
       "      <th>AGENT_KNOWLEDGE</th>\n",
       "      <th>VOLT_FLAG_BINARY</th>\n",
       "      <th>TREATMENT_BINARY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>control</td>\n",
       "      <td>NaN</td>\n",
       "      <td>352240580</td>\n",
       "      <td>10</td>\n",
       "      <td>Good package</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>control</td>\n",
       "      <td>yes</td>\n",
       "      <td>351664275</td>\n",
       "      <td>10</td>\n",
       "      <td>Very good customer service</td>\n",
       "      <td>Customer Service,General,UK Legacy</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>control</td>\n",
       "      <td>yes</td>\n",
       "      <td>351723391</td>\n",
       "      <td>10</td>\n",
       "      <td>So far so good. Charlie was very efficient and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>10</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>control</td>\n",
       "      <td>NaN</td>\n",
       "      <td>351702901</td>\n",
       "      <td>10</td>\n",
       "      <td>Great communication</td>\n",
       "      <td>Customer Service,General,UK Legacy</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>control</td>\n",
       "      <td>yes</td>\n",
       "      <td>352243612</td>\n",
       "      <td>10</td>\n",
       "      <td>Because Chris was amazing when she contacted m...</td>\n",
       "      <td>Customer Service,UK Legacy</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>10</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      GROUP VOLT_FLAG  SURVEY_ID  SCORE                                        LTR_COMMENT                      PRIMARY_REASON      MONTH  CONNECTION_TIME  SALES_PERSON_SAT  SALES_FRIENDLY_SAT  COMMINICATION_SAT  FIRST_BILL_SAT  AGENT_KNOWLEDGE  VOLT_FLAG_BINARY  TREATMENT_BINARY\n",
       "45  control       NaN  352240580     10                                       Good package                                 NaN 2023-03-01               10                10                   8                 10              10               10                 0                 0\n",
       "46  control       yes  351664275     10                         Very good customer service  Customer Service,General,UK Legacy 2023-03-01               10                10                  10                 10              10               10                 1                 0\n",
       "47  control       yes  351723391     10  So far so good. Charlie was very efficient and...                                 NaN 2023-03-01               10              <NA>                  10                 10              10               10                 1                 0\n",
       "48  control       NaN  351702901     10                                Great communication  Customer Service,General,UK Legacy 2023-03-01                9                10                  10                 10              10               10                 0                 0\n",
       "49  control       yes  352243612     10  Because Chris was amazing when she contacted m...          Customer Service,UK Legacy 2023-03-01               10              <NA>                  10                 10              10               10                 1                 0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f090f9d",
   "metadata": {},
   "source": [
    "#### Let's randomly sample some text responses to build a suitable approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "689d6891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: Very helpful staff\n",
      "\n",
      "Sample 2: Very polite engineer who consulted every step and very patient\n",
      "\n",
      "Sample 3: The experience is really good. The price can be a bit expensive. But I have had good experience with company media so far. The engineer, who came to my address was very professional and friendly. And did a really good job running the wires nice and tidy. Exactly how I wanted.\n",
      "\n",
      "Sample 4: Good value reasonable prices\n",
      "\n",
      "Sample 5: Very efficient and helpful\n",
      "\n",
      "Sample 6: The lady I spoke to was very helpful.\n",
      "\n",
      "Sample 7: Because it's too fast\n",
      "\n",
      "Sample 8: Just felt treated better than with sky\n",
      "\n",
      "Sample 9: Your customer service is awful and you really need to ditch the overseas call centers. Kindness and understanding in the voice go a long way. You can feel the contempt in operator voices (sniggering and laughing when you say you cannot understnd what they are saying - feels intentional) and cannot quite get all of the words due to very strong accents. Your only saving grace is the physical connection and stability of that. Thankfully, because connection quality is good I rarely (if ever - Thank Goodness) have to contact your call centers. Would not touch with a barge pole otherwise. Take a look at Now Broadband whos operators are absolutely marvellous. Lessons learned or brushed under the carpet? Haha\n",
      "\n",
      "Sample 10: Very friendly staff and accommodating\n",
      "\n",
      "Sample 11: The person who talked me through what was going to happen was very patient and talked me through all that wÃ¡s happening.\n",
      "\n",
      "Sample 12: Straight forward\n",
      "\n",
      "Sample 13: Friendly helpful\n",
      "\n",
      "Sample 14: Guys who rang me was very pleasant and he explained exactly what he was going to be doing as I wasn't at home at the time.\n",
      "\n",
      "Sample 15: The team who came out were friendly and explained all they needed to do. Also clear about the fact one of them was a trainee but still provided an excellent customer service\n",
      "\n",
      "Sample 16: Customer service excellent,polite and extremely helpful .\n",
      "\n",
      "Sample 17: I am new to company media so I can't say yet but I liked the service when setting up my account.\n",
      "\n",
      "Sample 18: The service was fabulous,\n",
      "\n",
      "Sample 19: Prompt response Swift delivery Good network Affordability\n",
      "\n",
      "Sample 20: company media is the worst company I have ever used you cut the phone off with out any notice so we only went bk with your terrible company to keep our phone number witch you promised we would and you even made a mess av that so now you've given us a new number absolutely disgusting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# randomly print a sample from non-missing LTR_COMMENT in df\n",
    "sample_size = 20\n",
    "sample = df[df['LTR_COMMENT'].notna()]['LTR_COMMENT'].sample(n=sample_size, random_state=46).tolist()\n",
    "for i, comment in enumerate(sample, 1):\n",
    "    print(f\"Sample {i}: {comment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8fdee",
   "metadata": {},
   "source": [
    "Comments comprise long and short responses. Topic modelling is more nuanced for longer responses, even though short responses can reflect similar (but more simple) themes. \n",
    "A suitable first step is to clean the responses to remove punctuation and unusual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8ba3f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the following as a module later on\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)  \n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "def clean_text(text, return_tokens=False, min_word_length=2, language='english'):\n",
    "    \"\"\"\n",
    "    Clean a single text string for NLP analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text string to clean\n",
    "    return_tokens : bool, default False\n",
    "        If True, returns list of tokens; if False, returns cleaned text string\n",
    "    min_word_length : int, default 2\n",
    "        Minimum word length to keep\n",
    "    language : str, default 'english'\n",
    "        Language for stopwords\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str or list\n",
    "        Cleaned text string or list of tokens\n",
    "    \"\"\"\n",
    "    # Initialize components\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Handle missing/empty text\n",
    "    if pd.isna(text) or not str(text).strip():\n",
    "        return [] if return_tokens else ''\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove punctuation and normalize whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize, filter stopwords and short words\n",
    "    tokens = [word for word in text.split() \n",
    "              if word not in stop_words and len(word) >= min_word_length]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens if return_tokens else ' '.join(tokens)\n",
    "\n",
    "def clean_text_series(series, return_tokens=False, min_word_length=2, language='english'):\n",
    "    \"\"\"\n",
    "    Apply text cleaning to a pandas Series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pandas.Series\n",
    "        Series containing text data\n",
    "    return_tokens : bool, default False\n",
    "        If True, returns list of tokens; if False, returns cleaned text string\n",
    "    min_word_length : int, default 2\n",
    "        Minimum word length to keep\n",
    "    language : str, default 'english'\n",
    "        Language for stopwords\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.Series\n",
    "        Series with cleaned text\n",
    "    \"\"\"\n",
    "    return series.apply(lambda x: clean_text(x, return_tokens, min_word_length, language))\n",
    "\n",
    "def add_cleaned_text_columns(df, text_columns, suffix='_clean', \n",
    "                           min_word_length=2, language='english', \n",
    "                           add_tokens=True, add_word_count=True):\n",
    "    \"\"\"\n",
    "    Add cleaned text columns to dataframe for multiple text columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe\n",
    "    text_columns : str or list\n",
    "        Column name(s) containing text to clean\n",
    "    suffix : str, default '_clean'\n",
    "        Suffix for cleaned text columns\n",
    "    min_word_length : int, default 2\n",
    "        Minimum word length to keep\n",
    "    language : str, default 'english'\n",
    "        Language for stopwords\n",
    "    add_tokens : bool, default True\n",
    "        Whether to add tokenized version\n",
    "    add_word_count : bool, default True\n",
    "        Whether to add word count column\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Dataframe with additional cleaned text columns\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Ensure text_columns is a list\n",
    "    if isinstance(text_columns, str):\n",
    "        text_columns = [text_columns]\n",
    "    \n",
    "    for col in text_columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Add cleaned text column\n",
    "        clean_col = f\"{col}{suffix}\"\n",
    "        df_result[clean_col] = clean_text_series(\n",
    "            df[col], return_tokens=False, \n",
    "            min_word_length=min_word_length, language=language\n",
    "        )\n",
    "        \n",
    "        # Add tokenized version\n",
    "        if add_tokens:\n",
    "            tokens_col = f\"{col}_tokens\"\n",
    "            df_result[tokens_col] = clean_text_series(\n",
    "                df[col], return_tokens=True,\n",
    "                min_word_length=min_word_length, language=language\n",
    "            )\n",
    "        \n",
    "        # Add word count\n",
    "        if add_word_count:\n",
    "            count_col = f\"{col}_word_count\"\n",
    "            df_result[count_col] = df_result[clean_col].str.split().str.len().fillna(0)\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "# Convenience function for common preprocessing pipeline\n",
    "def preprocess_text_for_modeling(df, text_columns, min_word_length=2, \n",
    "                                language='english', filter_empty=True):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for text modeling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe\n",
    "    text_columns : str or list\n",
    "        Column name(s) containing text to clean\n",
    "    min_word_length : int, default 2\n",
    "        Minimum word length to keep\n",
    "    language : str, default 'english'\n",
    "        Language for stopwords\n",
    "    filter_empty : bool, default True\n",
    "        Whether to add flag for non-empty text\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Preprocessed dataframe ready for modeling\n",
    "    \"\"\"\n",
    "    # Clean text\n",
    "    df_clean = add_cleaned_text_columns(\n",
    "        df, text_columns, min_word_length=min_word_length, \n",
    "        language=language, add_tokens=True, add_word_count=True\n",
    "    )\n",
    "    \n",
    "    # Add flags for non-empty text\n",
    "    if filter_empty:\n",
    "        if isinstance(text_columns, str):\n",
    "            text_columns = [text_columns]\n",
    "        \n",
    "        for col in text_columns:\n",
    "            clean_col = f\"{col}_clean\"\n",
    "            flag_col = f\"has_{col}\"\n",
    "            df_clean[flag_col] = (df_clean[clean_col].str.len() > 0)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "343a07ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LTR_COMMENT_CLEAN'] = clean_text_series(df['LTR_COMMENT'], return_tokens=False, min_word_length=2, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e077832f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ORIGINAL vs CLEANED COMMENTS ===\n",
      "\n",
      "--- Sample 1 ---\n",
      "ORIGINAL: Percy explained things clearly and understood from the start what I wanted. there was no hard sell. there was listening and understanding. top marks Percy and thank you\n",
      "CLEANED:  percy explained thing clearly understood start wanted hard sell listening understanding top mark percy thank\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 2 ---\n",
      "ORIGINAL: Excellent service lovely representative Mathius\n",
      "CLEANED:  excellent service lovely representative mathius\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 3 ---\n",
      "ORIGINAL: Very friendly staff and accommodating\n",
      "CLEANED:  friendly staff accommodating\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 4 ---\n",
      "ORIGINAL: Very difficult to talk to anyone, very expensive, always an IT problem, too many calls, lots of mistakes took place with my name, dates leaving or moving in, misleading advertising in such that it appears cheaper than what it actually costs. Sorry if that is too negative, however this is an accurate reflection of company. You do however have fast and mostly reliable internet\n",
      "CLEANED:  difficult talk anyone expensive always problem many call lot mistake took place name date leaving moving misleading advertising appears cheaper actually cost sorry negative however accurate reflection company however fast mostly reliable internet\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 5 ---\n",
      "ORIGINAL: Your customer service was so good to me\n",
      "CLEANED:  customer service good\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 6 ---\n",
      "ORIGINAL: Perfect\n",
      "CLEANED:  perfect\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 7 ---\n",
      "ORIGINAL: Very efficient and Friendly Service\n",
      "CLEANED:  efficient friendly service\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 8 ---\n",
      "ORIGINAL: Jake the representative for company was simply above and beyond helpful and done everything for me with ease\n",
      "CLEANED:  jake representative company simply beyond helpful done everything ease\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 9 ---\n",
      "ORIGINAL: Excellent service from the get go, the gent I spoke to on the phone, and the guy who did the installation were superb. I can¬\"t remember their name though.\n",
      "CLEANED:  excellent service get go gent spoke phone guy installation superb remember name though\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 10 ---\n",
      "ORIGINAL: All requirements were met with courtesy and helpfulness and promptness. I was informed of all I needed to know.\n",
      "CLEANED:  requirement met courtesy helpfulness promptness informed needed know\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Simple side-by-side comparison of original vs cleaned comments\n",
    "sample_size = 10\n",
    "\n",
    "# Get sample of non-missing comments\n",
    "sample_df = df[df['LTR_COMMENT'].notna()].sample(n=sample_size, random_state=42)\n",
    "\n",
    "print(\"=== ORIGINAL vs CLEANED COMMENTS ===\\n\")\n",
    "\n",
    "for i, (idx, row) in enumerate(sample_df.iterrows(), 1):\n",
    "    print(f\"--- Sample {i} ---\")\n",
    "    print(f\"ORIGINAL: {row['LTR_COMMENT']}\")\n",
    "    print(f\"CLEANED:  {row['LTR_COMMENT_CLEAN']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed912a06",
   "metadata": {},
   "source": [
    "We follow a standard text cleaning algorithm as exampled in the above. We also remove stop-words that contain no semantic information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118f0d6",
   "metadata": {},
   "source": [
    "#### BERT Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "68894326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (582, 16)\n",
      "Clean data shape: (349, 16)\n",
      "Number of comments for topic modeling: 349\n"
     ]
    }
   ],
   "source": [
    "# BERTopic Implementation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Load your data (assuming df is already loaded with LTR_COMMENT_CLEAN column)\n",
    "# df = pd.read_pickle('../data/processed/cleaned_call_script_data.pkl')\n",
    "\n",
    "# Step 1: Data preparation - filter out null/missing comments\n",
    "df_clean = (df\n",
    "    .loc[df['LTR_COMMENT_CLEAN'].notna()]  # Remove missing comments\n",
    "    .loc[df['LTR_COMMENT_CLEAN'].str.len() >= 20]  # Remove very short comments\n",
    "    .reset_index(drop=True)  # Reset index after filtering\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "print(f\"Original data shape: {df.shape}\")\n",
    "print(f\"Clean data shape: {df_clean.shape}\")\n",
    "print(f\"Number of comments for topic modeling: {len(df_clean)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476b1599",
   "metadata": {},
   "source": [
    "Should we also do lemmatization and stemming?\n",
    "\n",
    "Need to think carefully about removing stop words. For our topic extraction to be useful, we need to remove:\n",
    "1) Generic english function words\n",
    "2) Additional function words that come up often in our sample\n",
    "3) \n",
    "\n",
    "We need to be careful to strike a balance within our stop word filtering. Too much and we risk diluting the data too excessively, too little and we'll end up capturing topics that aren't particularly useful, or say much at all about the concrete "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eac844c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After preprocessing: 347 documents ready for topic modeling\n",
      "\n",
      "=== ORIGINAL vs CLEANED COMMENTS ===\n",
      "\n",
      "--- Sample 1 ---\n",
      "ORIGINAL: Great benefits but pricey\n",
      "CLEANED:  benefit pricey\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 2 ---\n",
      "ORIGINAL: Although the initial inquiry was very smooth problems have arisen and are ongoing\n",
      "CLEANED:  although initial inquiry problem arisen ongoing\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 3 ---\n",
      "ORIGINAL: Everything sorted easily\n",
      "CLEANED:  everything sorted easily\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 4 ---\n",
      "ORIGINAL: Z, who guided me through the process on the phone, was the epitome of empathetic warm professionalism. If all of your customer service personnel are as good as her and your products remain competitive, I see myself becoming a long-term loyal customer.\n",
      "CLEANED:  guided process phone epitome empathetic warm professionalism customer service personnel product remain competitive see becoming long term loyal customer\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 5 ---\n",
      "ORIGINAL: Amazing customer service by sade\n",
      "CLEANED:  customer service sade\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 6 ---\n",
      "ORIGINAL: You keep sending me messages for a completely different person, you tell me my equipment has been delivered, NO it hasn¬\"t, the emails have the totally wrong price on them so am NOT impressed at all, so where we go from here I don¬\"t know\n",
      "CLEANED:  keep sending message completely different person equipment delivered email totally wrong price know\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 7 ---\n",
      "ORIGINAL: Very helpful and efficient service and competitive broadband TV package\n",
      "CLEANED:  service competitive broadband tv package\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 8 ---\n",
      "ORIGINAL: Operator was difficult to understand, call took over one to register and complete the process\n",
      "CLEANED:  operator understand call took one register complete process\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 9 ---\n",
      "ORIGINAL: Very friendly engineer who answered our questions kindly and installed our system without hassle and on time.\n",
      "CLEANED:  engineer answered question kindly installed system hassle time\n",
      "--------------------------------------------------------------------------------\n",
      "--- Sample 10 ---\n",
      "ORIGINAL: They are helpful, reliable and stick to their work. The man who answered my request was caring and efficient. I will say more once my WiFi is installed but that will not be for another 2 weeks. I have used company Media for over 8 years!\n",
      "CLEANED:  stick work man answered request caring wifi installed another week used company medium year\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Text preprocessing - conservative stopword removal for short feedback\n",
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Conservative stopword removal - only remove highly frequent, non-topical words\n",
    "# KEEP: sentiment words (terrible, great, bad, lovely, thanks)\n",
    "# KEEP: action/domain words (helped, agent, product, resolved, call, phone)\n",
    "# REMOVE: only function words and universal domain terms if truly universal\n",
    "\n",
    "minimal_stopwords = {\n",
    "    # Core function words\n",
    "    'the', 'is', 'was', 'are', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
    "    'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must',\n",
    "    'shall', 'can', 'am', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', \n",
    "    'her', 'us', 'them', 'my', 'your', 'his', 'its', 'our', 'their', 'this', 'that',\n",
    "    'these', 'those', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
    "    'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before',\n",
    "    'after', 'above', 'below', 'between', 'among', 'within', 'without', 'against',\n",
    "    \n",
    "    # Additional function words spotted in your samples\n",
    "    'so', 'as', 'if', 'when', 'where', 'why', 'how', 'what', 'who', 'which', 'than',\n",
    "    'then', 'now', 'here', 'there', 'get', 'got', 'go', 'went', 'come', 'came',\n",
    "    'said', 'say', 'told', 'tell', 'also', 'really', 'very', 'quite', 'just', 'even',\n",
    "    'still', 'yet', 'already', 'always', 'never', 'sometimes', 'often',\n",
    "    \n",
    "    # Negative sentiment words\n",
    "    'bad', 'awful', 'terrible', 'horrible', 'shocking', 'disappointing', 'poor',\n",
    "    'unhelpful', 'rude', 'difficult', 'complicated', 'slow', 'delayed', 'frustrated',\n",
    "    'angry', 'upset', 'confused', 'misleading', 'expensive', 'overpriced', 'unfair',\n",
    "    'unacceptable', 'inadequate', 'unsatisfactory', 'problematic', 'troublesome',\n",
    "    'dishonest', 'unprofessional', 'incompetent', 'useless', 'pathetic', 'ridiculous',\n",
    "    \n",
    "    # Positive sentiment words\n",
    "    'good', 'great', 'excellent', 'amazing', 'fantastic', 'wonderful', 'brilliant', \n",
    "    'helpful', 'friendly', 'professional', 'efficient', 'polite', 'pleasant', \n",
    "    'straightforward', 'simple', 'easy', 'quick', 'fast', 'smooth', 'satisfied', \n",
    "    'happy', 'impressed', 'pleased', 'lovely', 'nice', 'perfect', 'outstanding',\n",
    "    'superb', 'awesome', 'terrific', 'exceptional', 'skilled', 'knowledgeable',\n",
    "    'patient', 'understanding', 'reassuring', 'confident', 'reliable', 'trustworthy',\n",
    "    \n",
    "    # Common non-adjectives\n",
    "    'the', 'and', 'for', 'ing', 'ed', 'ly', 'able', 'having', 'being', 'doing', \n",
    "    'going', 'coming', 'getting', 'looking', 'working', 'talking', 'calling',\n",
    "    'waiting', 'trying', 'running', 'using', 'taking', 'making', 'asked',\n",
    "    'said', 'told', 'found', 'needed', 'wanted', 'called', 'received'\n",
    "}\n",
    "\n",
    "# Only add domain-specific terms if they appear in ALL feedback and add no distinction\n",
    "# Be very conservative here - check your data first!\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Minimal preprocessing for short customer feedback\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Light cleaning - remove only obvious noise, keep most punctuation context\n",
    "    text = re.sub(r'[^\\w\\s\\']', ' ', text)  # Keep apostrophes for contractions\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Conservative stopword removal - only remove function words\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in minimal_stopwords and len(word) > 1]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "df_clean['text_processed'] = df_clean['LTR_COMMENT_CLEAN'].apply(preprocess_text)\n",
    "\n",
    "# Filter out empty processed texts\n",
    "df_clean = df_clean[df_clean['text_processed'].str.len() > 0].copy()\n",
    "\n",
    "# Prepare final documents for topic modeling\n",
    "docs = df_clean['text_processed'].tolist()\n",
    "\n",
    "print(f\"After preprocessing: {len(docs)} documents ready for topic modeling\")\n",
    "\n",
    "\n",
    "# Print comparison of original vs cleaned comments\n",
    "print(\"\\n=== ORIGINAL vs CLEANED COMMENTS ===\\n\")\n",
    "for i, (idx, row) in enumerate(df_clean.sample(n=10, random_state=42).iterrows(), 1):\n",
    "    print(f\"--- Sample {i} ---\")\n",
    "    print(f\"ORIGINAL: {row['LTR_COMMENT']}\")\n",
    "    print(f\"CLEANED:  {row['text_processed']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f5689",
   "metadata": {},
   "source": [
    "#### Why use BERT Model?\n",
    "\n",
    "Focus on embeddings makes it robust to typos and messiness of our raw data.\n",
    "Allows vector similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4739141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize BERTopic model\n",
    "# BERT embeddings and clustering\n",
    "topic_model = BERTopic(\n",
    "    verbose=True,\n",
    "    calculate_probabilities=True,  # Enable probability calculation\n",
    "    nr_topics='auto'  # Let BERTopic determine optimal number of topics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "319796ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 16:46:43,442 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting BERTopic model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9263fa756cd743fb93a1c97f835c499b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 16:46:45,252 - BERTopic - Embedding - Completed ✓\n",
      "2025-08-03 16:46:45,253 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-08-03 16:46:45,370 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-08-03 16:46:45,370 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-08-03 16:46:45,384 - BERTopic - Cluster - Completed ✓\n",
      "2025-08-03 16:46:45,385 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-08-03 16:46:45,394 - BERTopic - Representation - Completed ✓\n",
      "2025-08-03 16:46:45,395 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-08-03 16:46:45,397 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-08-03 16:46:45,405 - BERTopic - Representation - Completed ✓\n",
      "2025-08-03 16:46:45,406 - BERTopic - Topic reduction - Reduced number of topics from 7 to 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Fit the model and get topics\n",
    "print(\"Fitting BERTopic model...\")\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# Step 5: Add results back to dataframe\n",
    "df_clean = (df_clean\n",
    "    .assign(\n",
    "        topic=topics,\n",
    "        topic_prob=probs.max(axis=1) if hasattr(probs, 'max') else [max(p) for p in probs]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ef36a51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of topics generated: 7\n",
      "\n",
      "Topic Information:\n",
      "   Topic  Count                                     Name                                     Representation                                Representative_Docs\n",
      "0     -1     59      -1_agent_everything_package_service  [agent, everything, package, service, explaine...  [agent clearly stated everything know took who...\n",
      "1      0    152                 0_company_call_phone_day  [company, call, phone, day, service, broadband...  [placing order online free installation activa...\n",
      "2      1     58     1_service_customer_company_stressful  [service, customer, company, stressful, covera...  [customer service, customer service, customer ...\n",
      "3      2     27        2_explained_everything_well_fully  [explained, everything, well, fully, clara, ex...  [explained thing well, everything explained we...\n",
      "4      3     21  3_staff_process_product_professionalism  [staff, process, product, professionalism, kne...              [service staff, staff, process staff]\n",
      "5      4     15    4_communication_talked_daughter_thing  [communication, talked, daughter, thing, engli...  [daughter everything operator 8o year old unde...\n",
      "6      5     15         5_price_value_reasonable_service  [price, value, reasonable, service, experience...  [price experience service, service price, serv...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 6: Display topic information\n",
    "print(f\"\\nNumber of topics generated: {len(topic_model.get_topic_info())}\")\n",
    "print(\"\\nTopic Information:\")\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "637a9d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample documents by topic:\n",
      "\n",
      "--- Topic 0 ---\n",
      "1. chris amazing contacted put detail online...\n",
      "2. price bundle went day placed order...\n",
      "3. simple straightforward process lot information thing remember phone...\n",
      "\n",
      "--- Topic 1 ---\n",
      "1. good customer service...\n",
      "2. amazing service job completed...\n",
      "3. hood customer service...\n",
      "\n",
      "--- Topic 2 ---\n",
      "1. far good charlie efficient helpful let hope continues confident...\n",
      "2. nice pleasent aswell...\n",
      "3. alright reasonably good...\n",
      "\n",
      "--- Topic 3 ---\n",
      "1. simple process helpful staff...\n",
      "2. really friendly experienced staff...\n",
      "3. competent representative guide joining companyia medium...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 7: Show representative documents for each topic\n",
    "print(\"\\nSample documents by topic:\")\n",
    "for topic_id in topic_info['Topic'][:5]:  # Show first 5 topics\n",
    "    if topic_id != -1:  # Skip outlier topic\n",
    "        print(f\"\\n--- Topic {topic_id} ---\")\n",
    "        sample_docs = df_clean[df_clean['topic'] == topic_id]['LTR_COMMENT_CLEAN'].head(3)\n",
    "        for i, doc in enumerate(sample_docs, 1):\n",
    "            print(f\"{i}. {doc[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "32dc5572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original number of topics: 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 8: Topic reduction (if needed)\n",
    "print(f\"\\nOriginal number of topics: {len(topic_model.get_topic_info())}\")\n",
    "\n",
    "# Reduce topics if there are too many (following guide's example)\n",
    "if len(topic_model.get_topic_info()) > 20:\n",
    "    print(\"Reducing number of topics...\")\n",
    "    topic_model.reduce_topics(docs, nr_topics=15)\n",
    "    \n",
    "    # Update topics after reduction\n",
    "    topics_reduced = topic_model.topics_\n",
    "    df_clean['topic_reduced'] = topics_reduced\n",
    "    \n",
    "    print(f\"Reduced number of topics: {len(topic_model.get_topic_info())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ac350",
   "metadata": {},
   "source": [
    "Some things to consider:\n",
    "\n",
    "- We can remove all sentiment related words, does this improve concrete topic extraction?\n",
    "- Substantively, should our topics include sentiment related words? What's a justifiable methodology for our three questions?\n",
    "\n",
    "Should dedicate an entire section to outlining the methodology surrounding stop words. At times, adjectives are useful. And at times, they aren't.\n",
    "\n",
    "- It may be possible to do a continuous classification, rather than a discrete classification of each comment into every category. And then apply a threshold based on cosine similarity etc.\n",
    "\n",
    "Need to look into fine-tuning the BERTopic model or using a different approach, current topics just aren't substantively relevant at the moment. Need to fine-tune based on addressing the specific segmentation questions.\n",
    "\n",
    "Add an LLM-based implementation. May be more adaptive to the nuances of our data.\n",
    "\n",
    "Should also look into entity analysis, which can extract the proper nouns and things within each response, and then pass these onto the topic/theme model. [text](https://cloud.google.com/natural-language/docs/analyzing-entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
